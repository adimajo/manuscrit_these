%\chapter*{Conclusion: High dimensional unstructured data in \textit{Credit Scoring}} \label{ccl}
\chapter*{Conclusion and prospects} \label{ccl}

\selectlanguage{english}

\epigraph{It's not complicated, it's just a lot of it.}{Richard Feynman, interview for \textit{The World from Another Point of View}, Yorkshire Television, 1972.}

\minitoc

Various sources estimate the growth of created data to be exponential. However, the difficulty of processing these data has superseded the difficulty of storing them: ``data is the new oil'' is the catch-phrase often repeated in industry. While this oil has been extensively extracted and stored in a lot of application contexts, including \textit{Credit Scoring}, there is not always a motor capable of burning it. \textit{Scalability} refers to the problem of applying an existing method to increasingly more data. It turns out that, either by lack of computing power and / or by statistical properties or assumptions not met, not all methods are scalable.
Consequently, the statistics and machine learning communities have already tackled lots of problems stemming specifically from large $n$ and / or large $d$ settings.
These problems form a vast literature and are out of the scope of the present work.
The aim of these concluding remarks is to give a concise context of high-dimensional data w.r.t.\ the \textit{Credit Scoring} industry, what problems does it give rise to, and some simple existing solutions from an eluded literature review.

\section{Motivation}

This first section aims at presenting the data currently collected but remaining unused in the context of \textit{Credit Scoring} and the two sub-problems that were identified and tackled in this chapter.

\subsection{Industrial context}

Technological advancements in big data storage and processing has sparked interest in exploiting these for \textit{Credit Scoring}, although most hereafter presented data sources were available for quite some time\dots

\paragraph{Payment data}

Once a loan has been granted, monthly payments due by clients are most of the time debited from their main bank account. These debit might be accepted or refused by their bank depending on their balance and several tries might be performed before going into a recovery process. Such data are presented on Table~\ref{tab:payment_data}.

\begin{table}[ht]
    \centering
    \caption{Payment data.}
    \label{tab:payment_data}
    \begin{small}
\begin{tabular}{lllllll}
Client & Date & Should pay & Has paid & Type & Outstanding & Status \\
 \hline
1 & 05/01/2019:10:00:00 & 50 & 0 & Automatic debit & 5{,}000 & Refused \\
1 & 08/01/2019:10:00:00 & 50 & 50 & Automatic debit & 4{,}950 & Accepted \\
1 & 05/02/2019:10:00:00 & 50 & 0 & Automatic debit & 4{,}950 & Refused \\
1 & 08/02/2019:10:00:00 & 50 & 0 & Automatic debit & 4{,}950 & Refused
\end{tabular}
    \end{small}
\end{table}


\paragraph{Recovery data}

In the case of Client 1 from the previous example in Table~\ref{tab:payment_data}, once the second automatic debit is refused, it enters a recovery process that can be long and complex and is way out of the scope of the present manuscript. It creates however tremendous amounts of data, that could be used in the context of \textit{Credit Scoring}, \textit{e.g.}\ for better assessment of the class to predict (good / bad borrower) or as predictive features for a known client that applies for another loan.

\begin{table}[ht]
    \centering
    \caption{Monthly per-client recovery data.}
    \label{tab:recovery_data}
    \begin{small}
\begin{tabular}{lllllll}
Client & Date & Should pay & Fees & Has paid & Outstanding & Status \\
 \hline
1 & 09/02/2019:11:24:12 & 50 & 10 & 0 & 4{,}960 & Manual recovery by phone \\
1 & 09/02/2019:11:26:09 & 60 & 0 & 60 & 4{,}900 & Debit card payment \\
\end{tabular}
    \end{small}
\end{table}

\paragraph{Credit card data}

Transactions from credit card holders are recorded and can easily be retrieved. They are well-structured but contain lots of text fields, as exemplified on Table~\ref{tab:credit_card_data}.

\begin{table}[ht]
    \centering
    \caption{Daily per-client credit card data.}
    \label{tab:credit_card_data}
\begin{tiny}
\resizebox{\textwidth}{!}{\begin{tabular}{lllllll}
Client & Date & Amount & Company & Location & Category & \dots \\
 \hline
1 & 01/01/2019:09:05:18 & 10.9 & Amazon & Online & Online retail & \dots \\
1 & 01/01/2019:12:50:25 & 14.5 & Les 3 Brasseurs & 22 Place de la Gare, 59800 LILLE & Restaurant & \dots \\
1 & 02/01/2019:19:10:20 & 78.9 & Carrefour & 1 Avenue Willy Brandt, 59000 LILLE & Retail consumer goods & \dots 
\end{tabular}
}
\end{tiny}
\end{table}


\paragraph{Log data}

In the same fashion as social media users are targeted with personalized ads thanks to their visitation pattern~\cite{Yan:2017:YAY:3109859.3109923}, connexion logs can be used to personalize the loan offer in terms of amount, rate, \dots An example is given in Table~\ref{tab:log_data}.

\begin{table}[ht]
    \centering
    \caption{Log data.}
    \label{tab:log_data}
    \begin{small}
\begin{tabular}{lllllll}
Client & Platform & Device & Date & URL \\
 \hline
1 & Leboncoin & MAC OS & 10/12/2018:22:33:50 & /leboncoin/Nord/Electromenager/ \\
1 & Main site & MAC OS & 10/12/2018:22:34:10 & /sofinco/home \\
1 & Main site & MAC OS & 10/12/2018:22:34:30 & /sofinco/perso/electromenager \\
1 & Main site & MAC OS & 10/12/2018:22:35:12 & /sofinco/simulation \\
\end{tabular}
    \end{small}
\end{table}





\paragraph{Marketing data}

Finally, clients often apply to loans after having been exposed to diverse forms of adverts, some of which can be properly recorded and affected to a client, \textit{e.g.}\ mailing or e-mailing campaigns, Google AdWords, etc. An example of such data is visible on Table~\ref{tab:marketing_data}. These data can be very informative of the class (good / bad borrower) of each client: a prospective client coming from AdWords in the middle of the night might be riskier than a targeted prospect via an email on a week-end afternoon for example.

\begin{table}[ht]
    \centering
    \caption{Marketing data.}
    \label{tab:marketing_data}
\begin{tiny}
\resizebox{\textwidth}{!}{\begin{tabular}{lllllll}
Client & Marketing lever & Date & Device & Opened & Visited & URL \\
 \hline
1 & email & 11/12/2018:15:02:54 & Android & Yes & No & /media/new\_credit\_ad/car\_loan\&id=1\&\dots \\
1 & mail & 12/12/2018:10:00:00 & NA & NA & NA & NA \\
1 & Google Adword & 13/12/2019:12:10:10 & Windows & NA & Yes & /adword/personal\_credit\&id=1\&\dots \\
\end{tabular}}
\end{tiny}
\end{table}

All these kinds of data are not directly used by \gls{cacf} in its \textit{Credit Scoring} practices, although by simply looking at the exemplary tables, one is able to draw simple intuitions of signals of low / high risk of default. In the subsequent section, two problems pertaining the usage of these data, justifying in a sense why they were not used to this day, are identified and formalized.


\subsection{Two identified sub-problems}

A very simple way of dealing with all examples of additional data of the preceding Section is to add them as columns of our ``traditional'' data (displayed in Figure~\ref{tab:design} for example). Taking Table~\ref{tab:marketing_data} as an example, each marketing contact with the client can be reshaped so as to fit in separate columns relative to contact \#1, contact \#2, etc. This would yield Table~\ref{tab:example_longitudinal} and we could easily imagine appending to it log, credit card, recovery and payment data in a similar way. As a consequence, we are artificially back to a traditional setting with a very high number of covariates $d$. This setting is the subject of the next section. 

A probably clever way to use these data is to exploit their temporal structure, just as Recurrent Neural Networks have been able, on \textit{e.g.}\ sentiment analysis problems by analysing raw text, to perform better than methods not making use of this structure and requiring manual pre-processing such as n-grams~\cite{manning1999foundations}. However, such methods are hardly interpretable~\cite{lou2012intelligible}, which forced practitioners to resort to manual, intractable feature engineering, such as counting the number of credit card transactions over various time periods, which serve as inputs to simple models such as \gls{lr}. To avoid this time-consuming task without harming the interpretability of the resulting model nor its performance, and similarly to the quantization approach developed in Chapter~\ref{chap4}, suitable structured representations of these data have to be automatically extracted. Some techniques are provided in the subsequent section.

\begin{table}[ht]
    \centering
    \caption{Long data.}
    \label{tab:example_longitudinal}
    \begin{tiny}
\begin{tabular}{llllllllll}
Client & Job & Children & Marketing lever 1 & Date 1 & Device 1 & Marketing lever 2 & Date 2 & Device 2 & \dots \\
 \hline
1 & Skilled worker & 1 & email & 02/03/2019:15:02:54 & Android & Google Adword & 04/03/2019:12:01:01 & Windows & \dots \\
2 & Technician & 3 & mail & 02/04/2019:10:00:00 & NA & NA & NA & \dots\\
3 & Executive & 0 & Google Adword & 15/04/2019:12:10:10 & Windows & mail & 01/05/2019:10:00:00 & NA & \dots \\
\end{tabular}
    \end{tiny}
\end{table}


\section{Longitudinal data in high dimension}

This section is an eluded literature review of problems that arise in high dimension for ``classical'' data. It was first tackled by bio-statisticians working with omics data, such as DNA that can span over thousands of features for each patient, which yields a situation where more features than observations are available!

\textbf{Remarque :} les parties suivantes sont incomplètes à ce jour.

\subsection{Remark on the $d > n$ setting}

A lot of work has been dedicated to this setting (see~\cite{buhlmann2011statistics} for a review) since a lot of classical statistical methods do not work out-of-the box, including \gls{lr}. Independently from their relative consistency properties on selecting the ``best'' features, penalization methods naturally select at most $d$ features (whatever the amount of penalization)~\cite{zhu2004classification} such that we can assume in what follows $d \leq n$.
% Recall from Section~\ref{subsec:gradient} that the \gls{lr} coefficient $\gls{bth}$ is iteratively calculated from $ $ which is not defined for $d > n$.

\setlength{\epigraphwidth}{0.8\textwidth}

In the next section, we review the statistical properties associated with the ``curse of dimensionality'', a term attributed to Bellman:
\epigraph{All [problems due to high dimension] may be subsumed under the heading “the curse of dimensionality”. Since this is a curse, [...], there is no need to feel discouraged about the possibility of obtaining significant results despite it.}{R. Bellman, ``Dynamic programming'', 1957}

\subsection{The curse of dimensionality}

The foundation of statistics is that by having enough observations of random variables, we can approximate well (possibly intractable) integrals of their (possibly unknown) \gls{pdf} by an empirical average. Thus, a major problem in high dimensions is their relative emptiness, which makes the use of averages obsolete. Two classical examples are usually given: first, suppose your data lives in $[0,1]^d$ and you want to cover a neighbourhood of the origin of volume $v < 1$, \textit{e.g.}\ to perform nearest-neighbours classification. You want to know which fraction $s$ of each dimension needs to be covered. This fraction is given by $s = v^{1/d}$, such that for example a volume $v=0.5$ on a square is covered by $s = \sqrt{2}/2 \approx 0.71$, on a cube by $s = \sqrt[3]{0.5} \approx 0.79$, and so on. In high dimensional spaces such that $d>>1$, this fraction $s$ is approximately $1$ for every $v > \epsilon$: hence, neighbourhoods are not local anymore. Second, and somewhat subsequently to this first remark, the expected squared euclidean distance between two independent variables drawn uniformly in $[0,1]^d$ is $d/6$ as illustrated on Figure~\ref{fig:distance}. Consequently, it is often concluded that high dimension spaces are ``empty'' since points are all far away from each others.

\begin{figure}[!ht]
\centering
\resizebox{\textwidth}{!}{\input{R_CODE_FIGURES/chapitre7/freq_distance7.tex}}
\caption{Distribution of the euclidean distance between two random points of $[0,1]^d$ w.r.t.\ the dimension of the space $d \in \{ 2, 5, 10, 20, 50, 100, 1000 \}$.}
\label{fig:distance}
\end{figure}

%Coming back to the supervised classification setting, .

\subsection{The blessings of dimensionality}

The relative ``emptiness'' of the feature space $\gls{Xspace}$ in high dimension is not solely a curse: recall that we motivated the use of \gls{lr}, quantization and \gls{lr} trees for their interpretability. We would like simple, linear boundaries between good and bad borrowers. As should now be apparent from the few Gini figures given in this manuscript, such boundaries do not exist in \textit{Credit Scoring} in small dimension which motivates the use of additional data. The higher the dimension $d$, the higher the likelihood of existence of a linear boundary between good and bad borrowers~\cite{gorban2018blessing}. However, if some feature are just ``noise'' w.r.t.\ the class or contain redundant information, this linear boundary is highly likely to not generalize well, \textit{i.e.}\ overfit~\cite{bertrandagnan_2017}. To avoid this pitfall but still benefit from the vast amount of available data, a simple solution is to reduce the dimensionality $d$ of the data to only ``useful'' dimensions (in the sense of the predictive task).

\subsection{Dimension reduction}

A straightforward way of avoiding the curse(s) of dimensionality for \gls{lr} is to get back to a small dimension $d'$ relative to $n$ by pre-processing the $d$ features. In Chapter~\ref{chap4}, and particularly Section~\ref{sec:motivation}, it was argued that quantization could be thought of as a dimensionality reduction technique, because information was compressed in intervals and ``meta''-groups for continuous and categorical features respectively without affecting predictive power (on the contrary!). Two way more classical ways of performing dimensionality reduction are presented here: combining original features in principal components, which was already discussed in Chapter~\ref{chap6} when building segments of clients, and feature selection, which are the subjects of the two subsequent sections respectively.

\subsubsection{By combining input features}

Various algorithms were designed to map features onto a new ``representation'' which can have interesting properties. In Chapter~\ref{chap6}, we discussed at great length of \gls{famd} which consists in constructing orthogonal principal components, ranked by their respective eigen value which corresponds to the proportion of the total explained variance. In the same fashion as \textit{Credit Scoring} practitioners only care about the first two axes when looking for potential segments (see Chapter~\ref{chap6}), one could discard many dimensions of its original data by considering only the principal components that explain at least a given proportion of the total variance.

However, as was sensed in Chapter~\ref{chap6} for segmentation, the goal remains to predict a class, not to account for most of the variance of the predictors! These two goals being possibly disjoint, we introduced the \gls{pls} and \gls{spc} algorithms, which aim to take into account the target feature. Moreover, the \gls{spc} algorithm is an iterative procedure to select only principal components that have predictive power.


\subsubsection{By selecting input features}

Practitioners are often not convinced by the preceding approach since the new representation of the data is hard to grasp. Subsequently, feature selection approaches, which remain in the canonical space, are usually preferred. Such algorithms are out of the scope of the present manuscript, although the LASSO was mentioned earlier. This penalization method performs feature selection as a side effect and one of its refinement, the adaptive LASSO~\cite{zou2006adaptive}, has strong oracle properties (\textit{e.g.}\ it finds the subset of the features that participate in the true model, if it exists) which are appealing to both academics and industrial practitioners.

\section{New data types in a supervised classification setting}

In essence, it is not solely the volume of data that has to be addressed, but the variety of its format, as is apparent from the motivational section. These unstructured data require specific modelling techniques, ideally to automatically extract their predictive information into inputs that can be processed by simple interpretable models like \gls{lr}. Internally at \gls{cacf}, some simple features are extracted, typically from the credit card transactions, and seems to be the case in other financial institutions~\cite{verkade_2018}. In this applied work, the author compares the ``traditional'' approach to a model exploiting only similarities between clients' credit card transactions, achieving a good overall performance, in particular in conjunction with traditional data. This is not the case at \gls{cacf} where attempts to use only web logs showed poor performance. This empirical study motivated \gls{cacf} to study ways to structure these data and extract only the most important predictive information.

Is this structuring work a result of some statistical procedure or shall it remain a manual feature engineering work done by field experts? This question has found a clear answer in favour of automatic statistical procedures in fields like Computer Vision and Speech Recognition where neural networks, which are motivated by their automatic feature engineering, have made tremendous improvements over previous approaches relying on manual feature engineering~\cite{goodfellow2016deep}. These models are not directly applicable to \textit{Credit Scoring} since we require interpretability through simple models like \gls{lr}. Applying techniques from metric learning~\cite{2015BelletHS} and functional principal components~\cite{functional} are future areas of research, since credit card transactions can be reduced to a similarity measure as in~\cite{verkade_2018} and web visitation patterns can be seen as functional categorical data (where categories are web pages) that can be summarized by (functional) principal components.


\selectlanguage{french}

\section{Conclusion générale}

Cette thèse a permis d'explorer cinq sujets directement inspirés de problématiques industrielles de \textit{Credit Scoring}, sans doute graduellement du plus opérationnel, dont le questionnement était parfaitement posé, la ``réintégration des refusés'', au plus ouvert, l'utilisation de données non structurées en grande dimension, pour laquelle il ne semble pas y avoir d'approche universelle existante. On passe rapidement en revue ces problèmes en donnant les idées clés du problème, de sa résolution et des contributions de cette thèse.

\medskip

Le chapitre~\ref{chap2} consacré à la ``réintégration des refusés'' a permis de poser un problème ancien de l'industrie du crédit à la consommation : l'ensemble d'apprentissage de la règle de classement bons / mauvais payeurs est un échantillon de la population ayant déjà été financée. Ce financement est fortement corrélé à plusieurs règles existantes destinées à ne financer que des clients supposés bons. Cela induit-il un biais dans l'estimation des modèles de classification supervisée, en particulier la régression logistique ? En réinterprétant la classe des clients non financés comme des données manquantes, et en distinguant les cas du vrai (\textit{well-specified}) et du mauvais (\textit{misspecified}) modèle, on a montré que le paramètre de la régression logistique peut en effet être biaisé. Néanmoins, en reformulant les techniques \textit{ad hoc} d'utilisation des informations des clients non financés comme des tentatives de modélisation du mécanisme de financement, on a montré que la méthode actuelle consistant à n'utiliser que les clients financés pour lesquels $Z = \text{f}$ était satisfaisante.

\medskip

Rassuré sur la pertinence de l'échantillon d'apprentissage, le praticien poursuit ses travaux par certains pré-traitements, qui ont une justification pratique mais aussi théorique : apprendre une ``meilleure'' représentation des données au sens de l'interprétation du modèle mais aussi de sa qualité prédictive. La quantification (\textit{quantization}) regroupe la discrétisation de prédicteurs continus (la transformation d'un âge en une tranche d'âge par exemple) et le regroupement de modalités de prédicteurs catégoriels (le regroupement de modèles de véhicule en segments comme les citadines, routières, etc.). Ce pré-traitement manuel est à faible valeur ajoutée pour le statisticien et lui prend un temps considérable, qui tend à augmenter (du fait de l'augmentation du nombre de prédicteurs) ; de plus, en reposant sur des méthodes \textit{ad hoc} et univariées, la qualité prédictive du modèle résultant est diminuée. Il s'agissait alors de formaliser ce problème et de proposer une automatisation qui faisait néanmoins sens du point de vue statistique. Une nouvelle méthode, \textit{glmdisc}, est proposée, ainsi que deux stratégies d'estimation différentes, dont les résultats sur données réelles sont meilleurs que les approches \textit{ad hoc} susmentionnées et les méthodes d'état de l'art.

\medskip

De manière similaire, pour des raisons pratiques et théoriques, il est courant d'étudier des croisements (\textit{pairwise interactions}) de variables~: on suppose que l'effet combiné de deux prédicteurs sur le risque du client est différent de la somme des effets de ces prédicteurs. Encore une fois, des techniques \textit{ad hoc}, sous-optimales, étaient employées, nécessitant des données préalablement quantifiées. Or, la quantification et l'introduction d'interactions, en agissant sur l'espace des modèles considérés, doivent être effectuées simultanément. Une approche de type MCMC, utilisant l'algorithme de Metropolis-Hastings, a été proposée pour l'introduction d'interactions et dont l'intérêt principal est l'utilisation aisée en combinaison de l'algorithme \textit{glmdisc} construit pour le problème de quantification. Il est alors possible d'obtenir une régression logistique performante et interprétable en quelques heures de temps machine, ce qui nécessitait un à deux mois de temps humain.

\medskip

Nous avions ensuite pris du recul sur le quotidien du praticien en \textit{Credit Scoring} qui se voit confier des scores et / ou des améliorations ``locales'' du système d'acceptation, c'est-à-dire ne concernant qu'une partie de la population totale des demandeurs de crédit. En effet, ledit système est bien souvent composé de nombreuses règles ``métier'' (écartées de cette étude) mais surtout de nombreux scores, c'est-à-dire des régressions logistiques utilisant des variables différentes, des quantifications et croisements différents, et utilisées sur des clientèles différentes. En ne remettant jamais en cause la structure d'arbre du système d'acceptation total, la qualité prédictive est nécessairement sous-optimale, ce qui nous a conduit à présenter les méthodes actuelles utilisées en industrie pour construire des segments sur lesquelles différentes régressions logistiques sont ensuite construites. Plusieurs méthodes alternatives de l'état de l'art, très simples à mettre en oeuvre et qui produisent de meilleurs résultats que l'approche actuelle sur des données simulées, ainsi qu'une piste de résolution, sous la forme d'un algorithme~\gls{sem} comparable à celui exploité dans \textit{glmdisc} ont été passées en revue et comparées sur des données simulées et réelles et montrent de bons résultats préliminaires.
%L'application de cette piste de recherche sur les données réelles de \gls{cacf} est un premier futur axe de travail.

\medskip

Enfin, tous ces travaux exploitaient des données dites ``classiques'' en \textit{Credit Scoring}, c'est-à-dire majoritairement issues de formulaires remplis par le client ou par le vendeur (en magasin). \gls{cacf} dispose par ailleurs d'autres données, dont l'intérêt, la capacité prédictive additionnelle en tête de liste, reste à démontrer, comme par exemple les données transactionnelles de cartes de crédit, les données de log de connexion au site internet, les données marketing, etc. La présente conclusion a été l'occasion de voir les problèmes classiques liés à l'augmentation de dimension~: des espaces vides où la notion de ``voisin'' ne fait pas toujours sens, mais où il est plus facile de trouver de simples hyperplans séparant les classes bons et mauvais payeurs, pourvu que toutes ces nouvelles dimensions apportent de l'information. On s'est ensuite attardé sur les données non structurées à la disposition des banques dont la granularité fine (des centaines de transactions de carte bancaire pour un seul client) conduit le praticien, une fois de plus, à s'engouffrer dans des techniques \textit{ad hoc}, manuelles et chronophages d'agrégation, sans garantie statistique. La littérature relative à ces nouvelles données a été synthétisée de manière à favoriser la diffusion de ces bonnes pratiques~; l'application de ces techniques sur les données réelles de \gls{cacf} est un futur axe de travail.

\medskip

Pour conclure, cette thèse a permis d'apporter des réponses théoriques à des problèmes récurrents connexes au \textit{Credit Scoring} et nécessitant un tel travail de formalisation. Elle a également permis, s'agissant d'une thèse CIFRE, d'apporter une solution pratique aux problèmes de quantification et de croisements de variables sous la forme de deux solutions logicielles. Le chapitre~\ref{chap1} a permis d'introduire plusieurs problèmes ouverts liés au \textit{Credit Scoring}, parmi lesquels la ``segmentation'' et l'utilisation de données massives non structurées. Ces deux sujets ont fait l'objet des derniers travaux et ont abouti à une bibliographie épurée ainsi qu'à des simulations donnant une base solide à de futurs travaux. Les perspectives de travaux de recherche applicables au \textit{Credit Scoring} ne se tariront pas de si tôt, tant le contexte de disponibilité de nombreuses sources de données et les enjeux économiques importants sont catalyseurs des besoins de traitements statistiques rigoureux.

\printbibliography[heading=subbibliography, title=Références de la conclusion]