\chapter*{Conclusion}

% français

Cette thèse a permis d'explorer cinq sujets directement inspirés de problématiques industrielles de \textit{Credit Scoring}, sans doute graduellement du plus opérationnel, dont la problématique était parfaitement posée, la ``réintégration des refusés'', au plus ouvert, l'utilisation de données ``complexes'' et encore relativement nouvelles, pour laquelle il ne semble pas y avoir d'approche universelle existante. On passe rapidement en revue ces problèmes en donnant les idées clés du problème, de sa résolution et des contributions de cette thèse.

\medskip

Le chapitre~\ref{chap2} consacré à la ``réintégration des refusés'' a permis de poser un problème ancien de l'industrie du crédit à la consommation : l'ensemble d'apprentissage de la règle de classement bons / mauvais payeurs est un échantillon de la population ayant déjà été financée (ce qui est fortement corrélé à plusieurs règles destinées à ne financer que des bons clients, dont l'ancienne règle) ; cela induit-il un biais dans l'estimation des modèles de classification supervisée ? En réinterprétant les clients non financés comme des données manquantes, et en distinguant les cas du vrai (\textit{well-specified}) et du mauvais (\textit{misspecified}) modèle, on a montré que le paramètre de la régression logistique peut en effet être biaisé. Néanmoins, en reformulant les techniques \textit{ad hoc} d'utilisation des informations des clients non financés comme des tentatives de modélisation de la loi générative (la loi jointe des covariables et de l'information bon / mauvais payeur $p(\glssymbol{bx},y)$ par opposition à la loi prédictive $p(y | \glssymbol{bx})$), on a montré que la méthode actuelle consistant à n'utiliser que les clients financés pour lesquels $Z = \text{f}$ était la plus satisfaisante.

\medskip

Rassurés sur la pertinence de l'échantillon d'apprentissage, le praticien poursuit ses travaux par certains pré-traitements, qui ont une justification pratique mais aussi théorique : apprendre une ``meilleure'' représentation des données au sens de l'interprétation du modèle mais aussi de sa qualité prédictive. La quantification (\textit{quantization}) regroupe la discrétisation de prédicteurs continus (la transformation d'un âge en une tranche d'âge par exemple) et le regroupement de modalités de prédicteurs catégoriels (le regroupement de modèles de véhicule en segments comme les citadines, routières, etc.). Ce pré-traitement manuel est à faible valeur ajoutée pour le statisticien et lui prend un temps considérable, qui tend à augmenter (du fait de l'augmentation du nombre de prédicteurs) ; de plus, en reposant sur des méthodes \textit{ad hoc} et univariées, la qualité prédictive du modèle résultant est diminuée. Il s'agissait alors de formaliser ce problème, de proposer une automatisation qui faisait néanmoins sens du point de vue statistique. Une nouvelle méthode, \textit{glmdisc}, est proposée, ainsi que deux stratégies d'estimation différentes, dont les résultats sur données réelles sont meilleurs que les approches \textit{ad hoc} susmentionnées et les méthodes d'état de l'art.

\medskip

De manière similaire, pour des raisons pratiques et théoriques, il est courant d'étudier des croisements (\textit{pairwise interactions}) de variables~: on suppose que l'effet combiné de deux facteurs sur le risque du client est différent de la somme des effets de ces facteurs. Encore une fois, des techniques \textit{ad hoc} et univariées, sous-optimales, étaient employées et pré-supposaient des données quantifiées. Or, la quantification et l'introduction d'interactions, en agissant sur l'espace des modèles considérés, doivent être effectuées simultanément. Une approche de type MCMC, utilisant l'algorithme de Metropolis-Hastings, a été proposée pour l'introduction d'interactions et dont l'intérêt principal est l'utilisation aisée en combinaison de l'algorithme \textit{glmdisc} construit pour le problème de quantification. Il est alors possible d'obtenir une régression logistique performante et interprétable en quelques heures de temps machine, ce qui nécessitait un à deux mois de temps humain.

\medskip

Nous avions ensuite pris du recul sur le quotidien du praticien en \textit{Credit Scoring} qui se voit confier des scores et / ou des améliorations ``locales'' du système d'acceptation, c'est-à-dire ne concernant qu'une partie de la population totale des demandeurs de crédit. En effet, ledit système est bien souvent composé de nombreuses règles ``métier'' (écartées de cette étude) mais surtout de nombreux scores, c'est-à-dire des régressions logistiques utilisant des variables différentes, des quantifications et croisements différents, et utilisées sur des clientèles différentes. En ne remettant jamais en cause la structure d'arbre du système d'acceptation total, la qualité prédictive est nécessairement sous-optimale, ce qui nous a conduit à présenter les méthodes actuelles utilisées en industrie pour construire des segments sur lesquelles différentes \gls{lr} sont ensuite construites, plusieurs méthodes alternatives de l'état de l'art, très simples à mettre en oeuvre et qui produisent de meilleurs résultats que l'approche actuelle sur des données simulées, ainsi qu'une piste de résolution, sous la forme d'un algorithme~\gls{sem} comparable à celui exploité dans \textit{glmdisc} ont été passées en revue et comparées sur des données simulées. L'application de cette piste de recherche sur les données réelles de \gls{cacf} est un premier futur axe de travail.

\medskip

Enfin, tous ces travaux exploitaient des données dites ``classiques'' en \textit{Credit Scoring}, c'est-à-dire majoritairement issues de formulaires remplis par le client ou par le vendeur (en magasin). \gls{cacf} dispose par ailleurs d'autres données, dont l'intérêt, la capacité prédictive additionnelle en tête de liste, reste à démontrer, comme par exemple les données transactionnelles de cartes de crédit, les données de log de connexion au site internet, les données marketing, etc. Le chapitre~\ref{chap7} a été l'occasion de voir les problèmes classiques liés à l'augmentation de dimension dans un cadre ``longitudinal''~: des espaces vides où la notion de ``voisin'' ne fait pas toujours sens, mais où il est plus facile de trouver de simples hyperplans séparant les classes bons et mauvais payeurs, pourvu que toutes ces nouvelles dimensions apportent de l'information. On s'est ensuite attardé sur la notion de ``new'' data, ces données dont la granularité fine (des centaines de transactions de carte bancaire pour un seul client) et le caractère non-structuré (succession de paramètres Javascript dans une URL) conduisent les praticiens, une fois de plus, à s'engouffrer dans des techniques \textit{ad hoc}, manuelles et chronophages d'agrégation (nombre d'opérations portées au débit pendant 6 mois), sans garantie statistique. La littérature relative à ces nouvelles données a été synthétisée de manière à favoriser la diffusion de ces bonnes pratiques~; l'application de ces techniques sur les données réelles de \gls{cacf} est un deuxième futur axe de travail.

\medskip

Pour conclure, cette thèse a permis d'apporter des réponses théoriques à des problèmes récurrents connexes au \textit{Credit Scoring} et nécessitant un tel travail de formalisation. Elle a également permis, s'agissant d'une thèse CIFRE, d'apporter une solution pratique aux problèmes de quantification et de croisements de variables sous la forme de deux solutions logicielles. Le chapitre~\ref{chap1} a permis d'introduire plusieurs autres problèmes ouverts liés au \textit{Credit Scoring}, parmi lesquels la ``segmentation'' et l'utilisation de ``Big'' mais surtout ``new data'' dans leur forme et granularité. Ces deux sujets ont fait l'objet des derniers travaux et ont abouti à une bibliographie épurée ainsi que de travaux de simulation donnant une base solide à de futurs travaux. Les perspectives de travaux de recherche applicables au \textit{Credit Scoring} ne se tariront pas de si tôt, tant le contexte de disponibilité de nombreuses sources de données et les enjeux économiques importants sont catalyseurs des besoins de traitements statistiques rigoureux.