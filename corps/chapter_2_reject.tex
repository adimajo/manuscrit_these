\selectlanguage{english}

\chapter{Reject Inference: a rational review} \label{chap2}

\epigraph{Sounds good, doesn't work.}{Donald J.\ Trump}

\minitoc

%\textit{Nota Bene :} ce chapitre s'inspire fortement de l'article [...]

\bigskip

The granting process of all credit institutions is based on the probability that the applicant will refund his loan given his characteristics. This probability also called \gls{score} is learnt based on a dataset in which rejected applicants are \textit{de facto} excluded (see Section~\ref{subsec:critere}). This implies that the population on which the \gls{score} is used will be different from the learning population. Thus, this biased learning can have consequences on the scorecard's relevance. Many methods dubbed ``reject inference'' have been developed in order to try to exploit the data available from the rejected applicants to build the score. However most of these methods are considered from an empirical point of view, and there is some lack of formalization of the assumptions that are really made, and of the theoretical properties that can be expected. We propose a formalisation of these usually hidden assumptions for some of the most common reject inference methods, and we discus the improvement that can be expected. These conclusions are illustrated on simulated data and on real data from \gls{cacf}.

\section{Introduction}

In consumer loans, the acceptance process was described in Chapter~\ref{chap1} and can be formalized as follows. For a new applicant's profile and credit's characteristics, the lender aims at estimating the repayment probability. To this end, the \textit{credit modeler} fits a predictive model, often a \gls{lr}, between already  financed  clients' characteristics $\gls{bx}=(\gls{x}_1,\ldots,\gls{x}_d)$ and their repayment status, a binary variable $\gls{y}\in\{0,1\}$ (where $1$ corresponds to good clients and $0$ to bad clients). The model is then applied to the new applicant and yields an estimate of its repayment probability, called \gls{score} after an increasing transformation (see Section~\ref{subsec:apprentissage}).
%In practice, an increasing transformation of this probability also called score is often considered.
Over some cut-off value of the \gls{score}, the applicant is accepter, except if further expert rules come into play as can be seen from Figure~\ref{fig:figure1}.

\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\center \includegraphics[width=5cm]{figures/chapitre2/schema.png}
\caption{Simplified Acceptance mechanism in~Crédit Agricole Consumer Finance}
\label{fig:figure1}

\end{minipage}%
\hfil \begin{minipage}[b]{0.5\linewidth}

\center \begin{tikzpicture}

    \foreach \start/\end/\middle/\percent/\anchor/\name in {
      0/90/30/financed/above/accepted,
      90/180/150/not-taken-up/above/accepted}
  {
    \draw[fill=green, thick] (0,0) -- (\end:2.4cm) arc (\end:\start:2.4cm)
      node at (\middle:1.3cm) {\percent};
    \draw (\middle:2.4cm) -- (\middle:3cm) node[\anchor] {\name};
  };
    
    \foreach \start/\end/\middle/\percent/\anchor/\name in {
      180/240/200/score/below/rejected,
      240/300/270/rules/below/rejected,
      300/360/340/operator/below/rejected}
  {
    \draw[fill=red, thick] (0,0) -- (\end:2.4cm) arc (\end:\start:2.4cm)
      node at (\middle:1.3cm) {\percent};
    \draw (\middle:2.4cm) -- (\middle:3cm) node[\anchor] {\name};
  };
\end{tikzpicture}
\caption{Simplified Acceptance status in Crédit Agricole Consumer Finance - scale relations not respected}
\label{fig:figure2}

\end{minipage}
\end{figure}
 


The through-the-door population (all applicants) can be classified into two categories thanks to a binary variable $z$ taking values in $\{\f,\nf\}$ where $\f$ stands for financed applicants (in \textcolor{green}{green} on Figure~\ref{fig:figure2}) and $\nf$ for not financed ones (in \textcolor{red}{red} on Figure~\ref{fig:figure2}). As the repayment variable $\gls{y}$ is unobserved for not financed applicants, credit scorecards are only constructed on financed clients' data but then applied to the whole  through-the-door population. The relevance of this process is a natural question which is dealt in the field of {reject inference}. The idea is to use the characteristics of not financed clients in the scorecard building process to avoid a population bias, and thus to improve the prediction on the whole through-the-door population. Such methods have been described in~\cite{RI6,saporta,banasik,economix}, and have also notably been investigated in~\cite{RI2} who first saw {reject inference} as a missing data problem. In~\cite{RI3}, the misspecified model case on real data is studied specifically and is also developed here.


In fact, it can be considered as a part of the semi-supervised learning setting, which consists in learning from both labelled and unlabelled data. However, in the semi-supervised setting~\cite{Chapelle:2010:SL:1841234} it is generally assumed that labelled data and unlabelled data come from the same distribution, which is rarely the case in \textit{Credit Scoring}. Moreover, the main use case of semi-supervised learning is when the number of unlabelled data is far larger than the number of labelled data, which is not the case in \textit{Credit Scoring} since the number of rejected clients and accepted clients is often balanced and depends heavily on the financial institution, the portfolio considered, \textit{etc}.


The purpose of the present chapter is twofold: a clarification of which mathematical hypotheses, if any, underlie those reject inference methods and a clear conclusion on their relevance. In Section~\ref{sec:criteres}, we present a criterion to assess a method's performance and discuss missingness mechanisms that characterize the relationship of $\gls{z}$ with respect to $\gls{bx}$ and $\gls{y}$. In Section~\ref{sec:methods_reject}, we go through some of the most common reject inference methods and exhibit their mathematical properties. 
To confirm our theoretical findings, we test each method on real data from Crédit Agricole Consumer Finance in Section~\ref{sec:num_exp_reject}.
Finally, some guidelines are given to practitioners in Section~\ref{sec:conclusion_reject}.
%\textcolor{blue}{CB : ajouter description partie 5 (conclusion)}

\section{\textit{Credit Scoring} modelling} \label{sec:criteres}

\subsection{Data} 

The decision process of financial institutions to accept a credit application is usually embedded in the  probabilistic framework. The latter offers rigorous tools for taking into account both the variability of applicants and the uncertainty on their ability to pay back the loan. In this context, the important term is $p(\gls{y}|\gls{bx})$, designing the probability that a new applicant (described by his characteristics $\gls{bx}$) will pay back his loan ($\gls{y}=1$) or not ($\gls{y}=0$). Estimation of $p(\gls{y}|\gls{bx})$ is thus an essential task of any \textit{Credit Scoring} process.

To perform estimation, a specific $n + n'$-sample $\gls{T}$ is available, decomposed into two disjoint and meaningful subsets, denoted by $\gls{Tf}$ and $\gls{Tnf}$ ($\gls{T}=\gls{Tf} \cup \gls{Tnf}$, $\gls{Tf} \cap \gls{Tnf}=\emptyset$). The first subset ($\gls{Tf}$) corresponds to $n$ applicants with features $\gls{bx}_i$ who have been financed ($\gls{z}_i={\text{f}}$) and, consequently, for who the repayment status $\gls{y}_i$ is known, with their respective matrix notation $\gls{bbx}_\f$, $\gls{bbz}_\f$ and $\gls{bby}_\f$. Thus, $\gls{Tf}=(\gls{bx}_i,\gls{y}_i,\gls{z}_i)_{i\in \gls{F}} = (\gls{bbx}_\f,\gls{bby}_\f,\gls{bbz}_\f)$ where $\gls{F}=\{i:\gls{z}_i={\text{f}}\}$ denotes the corresponding subset of indexes. The second subset ($\gls{Tnf}$) corresponds to $n'$ applicants with features $\gls{bx}_i$ who have {\it not} been financed ($\gls{z}_i={\text{nf}}$) and, consequently, for who the repayment status $\gls{y}_i$ is {\it unknown}, with their respective matrix notation $\gls{bbx}_{\text{nf}}$ and $\gls{bbz}_{\text{nf}}$. Thus, $\gls{Tnf}=(\gls{bx}_i,\gls{z}_i)_{i\in \gls{NF}} = (\gls{bbx}_{\nf}, \gls{bbz}_{\nf})$ where $\gls{NF}=\{i:z_i={\nf}\}$ denotes the corresponding subset of indexes. We notice that $\gls{y}_i$ values are excluded from the observed sample $\gls{Tnf}$, since they are missing. These data can be represented schematically as:

\[ \gls{T} = \begin{array}{c}
\gls{Tf} = \bigg( \; \tikzmarkin[hor=style green]{el02} \gls{bbx}_{\text{f}} \tikzmarkend{el02} \\
\\
\cup \\
\\
\gls{Tnf} = \bigg( \; \tikzmarkin[hor=style orange]{el-12} \gls{bbx}_{\text{nf}} \tikzmarkend{el-12} \end{array}
\left( \begin{array}{ccc}
%\rowcolor{red!20}
\tikzmarkin[hor=style green]{e1} \; \; x_{1,1} & \cdots & x_{1,d}  \\
 \vdots & \vdots & \vdots  \\
 x_{n,1} & \cdots & x_{n,d} \tikzmarkend{e1} \\
\tikzmarkin[hor=style orange]{e2} \; \; x_{n+1,1} & \cdots & x_{n+1,d}  \\
 \vdots & \vdots & \vdots \\
 x_{n+n',1} & \cdots & x_{n+n',d} \tikzmarkend{e2} \end{array} \right),
 \hspace{1.5cm}
 \begin{array}{c}
\tikzmarkin[hor=style green]{el11} \gls{bby}_{\text{f}} \tikzmarkend{el11} \\
\\
\\
\\
\tikzmarkin[hor=style orange]{el12} \gls{bby}_{\text{nf}} \tikzmarkend{el12}\end{array}
\left( \begin{array}{c}
\tikzmarkin[hor=style green]{e4} y_1 \\
\vdots \\
y_n \tikzmarkend{e4} \\ 
\tikzmarkin[hor=style orange]{e3} \text{NA} \\
\vdots \\
\text{NA} \tikzmarkend{e3} \end{array} \right) ,
 \hspace{1.5cm}
 \begin{array}{c}
\tikzmarkin[hor=style green]{el5} \gls{bbz}_{\text{f}} \tikzmarkend{el5} \\
\\
\\
\\
\tikzmarkin[hor=style orange]{el6} \gls{bbz}_{\text{nf}} \tikzmarkend{el6} \end{array}
\left( \begin{array}{c}
\tikzmarkin[hor=style green]{e8} \text{f} \\
\vdots \\
\text{f} \tikzmarkend{e8} \\ 
\tikzmarkin[hor=style orange]{e9} \text{nf} \\
\vdots \\
\text{nf} \tikzmarkend{e9} \end{array} \right)
 \hspace{0.2cm}
 \begin{array}{c}
\bigg). \\
\\
\\
\\
\bigg). \end{array}
\]


\subsection{General parametric model}

Estimation of $p(\gls{y}|\gls{bx})$ has to rely on modelling since the true probability distribution is unknown. Firstly, it is both convenient and realistic to assume that triplets in $\mathcal{T}_{\text{c}} = (\gls{bx}_i,\gls{y}_i,\gls{z}_i)_{1\le i\le n+n'}$ are all independent and identically distributed (i.i.d.), including the unknown values of $\gls{y}_i$ when $i\in \gls{NF}$. Secondly, it is usual and convenient to assume that the unknown distribution $p(\gls{y}|\gls{bx})$ belongs to a given parametric family $\{p_{\gls{bth}}(\gls{y}|\gls{bx})\}_{\gls{bth} \in\Theta}$, where $\Theta$ is the parameter space as was discussed in Chapter~\ref{chap1}. For instance, \gls{lr} is often considered in practice, even if we will be more general in this section. However, \gls{lr} will be important for other sections since some standard reject inference methods are specific to this family (Section~\ref{sec:methods_reject}) and numerical experiments (Section~\ref{sec:num_exp_reject}, Appendices~\ref{subsec:app_reject_sim}, and~\ref{subsec:app_reject_real_method}) will implement them.

As in any missing data situation (here $\gls{z}$ indicates if $\gls{y}$ is observed or not), the relative modelling process, namely $p(\gls{z}|\gls{bx},\gls{y})$, has also to be clarified. For convenience, we can also consider a parametric family $\{p_{\gls{phi}}(\gls{z}|\gls{bx},\gls{y})\}_{\gls{phi} \in \Phi}$, where $\gls{phi}$ denotes the parameter and $\Phi$ the associated parameter space of the financing mechanism. Note we consider here the most general missing data situation, namely a \glsfirst{mnar} mechanism~\cite{littlerubin}. It means that $\gls{z}$ can be stochastically dependent on some missing data $\gls{y}$, namely that $p(\gls{z}|\gls{bx},\gls{y})\neq p(\gls{z}|\gls{bx})$. We will discuss this fact in Section~\ref{sec:mechanisms}.

Finally, combining both previous distributions $p_{\gls{bth}}(\gls{y}|\gls{bx})$ and $p_{\gls{phi}}(\gls{z}|\gls{bx},\gls{y})$ leads to express the joint distribution of $(\gls{y},\gls{z})$ conditionally to $\gls{bx}$ as:
\begin{equation}\label{eq:generative}
p_{\gamma}(\gls{y},\gls{z}|\gls{bx}) = p_{\gls{phi}(\gamma)}(\gls{z}|\gls{y},\gls{bx})p_{\gls{bth}(\gamma)}(\gls{y}|\gls{bx})
\end{equation}
where $\{p_{\gamma}(\gls{y},\gls{z}|\gls{bx})\}_{\gamma \in \Gamma}$ denotes a distribution family indexed by a parameter $\gamma$ evolving in a space $\Gamma$. Here it is clearly expressed that both parameters $\gls{phi}$ and $\gls{bth}$ can depend on $\gamma$, even if in the following we will note shortly $\gls{phi}=\gls{phi}(\gamma)$ and $\gls{bth}=\gls{bth}(\gamma)$. In this very general missing data situation, the missing process is said to be {\it non-ignorable}, meaning that parameters $\gls{phi}$ and $\gls{bth}$ can be functionally dependent (thus $\gamma\neq (\gls{phi},\gls{bth})$). We also discuss this fact in Section~\ref{sec:mechanisms}.

\subsection{Maximum likelihood estimation} 
\label{sec:EM}

Mixing previous model and data, the maximum likelihood principle can be invoked for estimating the whole parameter $\gamma$, thus yielding as a by-product an estimate of the parameter $\gls{bth}$. Indeed, $\gls{bth}$ is of particular interest, the goal of the financial institutions being solely to obtain an estimate of $p_{\gls{bth}}(\gls{y}|\gls{bx})$. The observed log-likelihood can be written as:
\begin{equation}\label{eq:like.MNAR}
\ell(\gamma;\gls{T}) = \sum_{i\in\gls{F}}\ln p_{\gamma}(\gls{y}_i,\text{f} | \gls{bx}_i) + \sum_{i'\in\gls{NF}} \ln \left[ \sum_{\gls{y}\in\{0,1\}} p_{\gamma}(\gls{y},\text{nf} | \gls{bx}_{i'}) \right].
\end{equation}
Within this missing data paradigm, the EM algorithm (see~\cite{dempster1977maximum}) can be used: it aims at maximizing the expectation of the complete likelihood $\ell_c(\gamma;\mathcal{T}_{\text{c}})$ (defined hereafter) where $\mathcal{T}_{\text{c}} = \gls{T} \cup \gls{bby}_{\text{nf}}$ over the missing labels. Starting from an initial value $\gamma^{(0)}$, iteration $(s)$ of the algorithm is decomposed into the following two classical steps:
\paragraph{E-step} compute the conditional probabilities of missing $\gls{y}_i$ values:
\begin{equation}
t_{i\gls{y}}^{(s)} = p_{\gls{bth}(\gamma^{(s-1)})}(\gls{y}|\gls{bx}_i,\nf) = \frac{p_{\gamma^{(s-1)}}(\gls{y}, \nf|\gls{bx}_i)}{\sum_{\gls{y}' = 0}^{1} p_{\gamma^{(s-1)}}(\gls{y}', \nf|\gls{bx}_i)};
\end{equation}
\paragraph{M-step} maximize the conditional expectation of the complete log-likelihood:
\begin{equation}\label{eq:like.c}
\ell_c(\gamma;\mathcal{T}_{\text{c}}) = \sum_{i=1}^{n+n'}\ln p_{\gamma}(\gls{y}_i,\gls{z}_i | \gls{bx}_i) = \sum_{i=1}^{n}\ln p_{\gamma}(\gls{y}_i,\f | \gls{bx}_i) + \sum_{i=n+1}^{n+n'}\ln p_{\gamma}(\gls{y}_i,\nf | \gls{bx}_i);
\end{equation}
leading to:
\begin{align*}
\gamma^{(s)} & = \argmax_{\gamma \in \Gamma} \mathbb{E}_{\gls{bby}_{\text{nf}}} [\ell_c(\gamma;\mathcal{T}_{\text{c}}) | \gls{T},\gamma^{(s-1)}] \\
& = \argmax_{\gamma \in \Gamma} \sum_{i\in \gls{F}} \ln p_{\gamma}(\gls{y}_i, \f|\gls{bx}_i) +  \sum_{i'\in \gls{NF}}\sum_{\gls{y} = 0}^{1}t_{i'\gls{y}}^{(s)} \ln p_{\gamma}(\gls{y}, \nf | \gls{bx}_{i'}).
\end{align*}
Usually, stopping rules rely either on a predefined number of iterations, or on a predefined stability criterion of the observed log-likelihood.

\subsection{Some current restrictive missingness mechanisms}
\label{sec:mechanisms}

The latter parametric family is very general since it considers both that the missingness mechanism is \gls{mnar} and non-ignorable. But in practice, it is common to consider ignorable models for the sake of simplicity, meaning that $\gamma= (\gls{phi},\gls{bth})$. There exists also some restrictions to the \gls{mnar} mechanism. 

The first restriction to \gls{mnar} is the \glsfirst{mcar} setting, meaning that $p(\gls{z}| \gls{bx},\gls{y}) = p(\gls{z})$. In that case, applicants should be accepted or rejected without taking into account their descriptors $\gls{bx}$. Such a process is not realistic at all for representing the actual process followed by financial institutions. Consequently it is always discarded in \textit{Credit Scoring}.

The second restriction to \gls{mnar} is the \glsfirst{mar} setting, meaning that $p(\gls{z}| \gls{bx},\gls{y}) = p(\gls{z}|\gls{bx})$. The \gls{mar} missingness mechanism seems realistic for \textit{Credit Scoring} applications, for example when financing is based solely on a function of $\gls{bx}$, \textit{e.g.}\ in the case of a score associated to a cut-off, provided all clients' characteristics of this existing \gls{score} are included in $\gls{bx}$. It is a usual assumption in \textit{Credit Scoring} even if, in practice, the financing mechanism may depend also on unobserved features (thus not present in $\gls{bx}$), which is particularly true when an operator (see Figure~\ref{fig:figure1}) adds a subjective, often intangible, expertise. In the \gls{mar} situation the log-likelihood~(\ref{eq:like.MNAR}) can be reduced to:
\begin{equation}\label{eq:like.MAR}
\ell(\gamma;\gls{T}) = \ell(\gls{bth};\gls{Tf}) + \sum_{i=1}^{n+n'} \ln p_{\gls{phi}}(\gls{z}_{i} | \gls{bx}_{i}),
\end{equation}
with $\ell(\gls{bth};\gls{Tf})=\sum_{i\in\gls{F}}\ln p_{\gls{bth}}(\gls{y}_i | \gls{bx}_i)$.
Combining it with the ignorable assumption, estimation of $\gls{bth}$ relies only on the first part $\ell(\gls{bth};\gls{Tf})$, since the value $\gls{phi}$ has no influence on~$\gls{bth}$. In that case, invoking an EM algorithm due to missing data $\gls{y}$ is no longer required as will be made explicit in Section~\ref{subsec:strat1}.
In practice, if there are other features $\tilde{\gls{bx}}$ that are either present in the preceding scorecard or that are available to manual operators that influence the outcome $\gls{y}$ (see Figures \ref{fig:figure1} and \ref{fig:mar}), a \gls{mnar} missingness mechanism has to be assumed. However, as is generally assumed in Credit Scoring, the \gls{mar} mechanism can be safely hypothesized since features used in the scorecards do not differ much (all available features are usually retained) and lots of portfolios (\textit{e.g.}\ for low amounts) have almost-automatic acceptance rules (such that manual operators follow the decision of the scorecards and only control the adequacy between declarative features $\gls{bx}$ and supporting documents).

\begin{figure}
\begin{centering}
\begin{tikzpicture}
 
\tikzset{vertex/.style = {shape=circle,draw,minimum size=1.5em}}
\tikzset{edge/.style = {->,> = latex'}}
% vertices
\node[vertex] (y) at  (0,0) {$\gls{y}$};
\node[vertex] (x) at  (2,0) {$\gls{bx}$};
\node[vertex] (xc) at  (2,1) {$\tilde{\gls{bx}}$};
\node[vertex] (z) at (4,0) {$\gls{z}$};
 
edges
\draw[edge] (y) to (x);
\draw[edge] (xc) to (x);
\draw[edge] (xc) to (z);
\draw[edge] (x) to (z);
\draw[dashed] (y) to (xc);
 
\end{tikzpicture}
\end{centering}
\caption{\label{fig:mar} Dependencies between random variables $\gls{y}$, $\tilde{\gls{bx}}$, $\gls{bx}$ and $\gls{z}$}
\end{figure}

\subsection{Model selection} \label{subsec:model_selection}

At this step, several kinds of parametric model~(\ref{eq:generative}) have been assumed. It concerns obviously the parametric family $\{p_{\gls{bth}}(\gls{y}|\gls{bx})\}_{\gls{bth} \in\Theta}$, and also the missingness mechanism \gls{mar} or \gls{mnar}. 
However, it has to be noticed that \gls{mar} versus \gls{mnar} cannot be tested since we do not have access to $\gls{y}$ for not financed clients~\cite{molenberghs2008every}. However, model selection is possible by modelling also the whole financing mechanism, namely the family $\{p_{\gls{phi}}(\gls{z}|\gls{bx},\gls{y})\}_{\gls{phi} \in \Phi}$.


Scoring for credit application can be recast as a semi-supervised classification problem~\cite{Chapelle:2010:SL:1841234}. In this case, classical model selection criteria can be divided into two categories~\cite{vandewalle:tel-00447141}: either scoring performance criteria as \textit{e.g.}\ error rate on a test set $\gls{T}^{\text{test}}$, or information criteria like \textit{e.g.}\ BIC as was introduced in Section~\ref{subsubsec:choix_modele}.

In the category of error rate criteria, the typical error rate is expressed as follows:
\begin{equation} \label{eq:error}
\mbox{Error}(\gls{T}^\text{test}) = \frac{1}{|\gls{T}^\text{test}|} \sum_{i \in \gls{T}^\text{test}} \mathbb{I}(\hat y_i \neq y_i),
\end{equation} 
where $\gls{T}^\text{test}$ is an i.i.d.\ test sample from $p(\gls{y}|\gls{bx})$ and where $\hat{\gls{y}}_i$ is the estimated value of the related $\gls{y}_i$ value involved by the estimated model at hand. The model leading to the lowest error value is then retained. However, in the \textit{Credit Scoring} context this criterion family is not available since no sample $\gls{T}^\text{test}$ is itself available. This problem can be exhibited through the following straightforward expression
\begin{equation} \label{eq:error2}
p(\gls{y}|\gls{bx}) = \sum_{\gls{z}\in\{\text{f},\text{nf}\}} p(\gls{y}|\gls{bx},\gls{z}) p(\gls{z}|\gls{bx})
\end{equation}
where $p(\gls{y}|\gls{bx},\gls{z})$ is unknown and $p(\gls{z}|\gls{bx})$ is known since this latter is defined by the financial institution itself. We notice that obtaining a sample from $p(\gls{y}|\gls{bx})$ would require that the financial institution draws $\gls{bbz}^\text{test}$ i.i.d.\ from $p(\gls{z}|\gls{bx})$ before to observe the results $\gls{bby}^\text{test}$ i.i.d.\ from $p(\gls{y}|\gls{bx},\gls{z})$. But in practice it is obviously not the case, a threshold being applied to the distribution $p(\gls{z}|\gls{bx})$ for retaining only a set of fundable applicants, the non-fundable applicants being definitively discarded, preventing us from getting a test sample $\gls{T}^\text{test}$ from $p(\gls{y} | \gls{bx})$. As a matter of fact, only a sample $\gls{Tf}^\text{test}$ of $p(\gls{y}|\gls{bx},\text{f})$ is available, 
irrevocably prohibiting the calculus of~(\ref{eq:error}) as a model selection criterion.

In the category of information criteria, the BIC criterion (presented in Section~\ref{subsubsec:choix_modele}) is expressed as the following penalization of the maximum log-likelihood:
\begin{equation}\label{eq:AIC}
\mbox{BIC} = - 2 \ell(\hat\gamma;\gls{T}) + \text{dim}(\Gamma) \ln n,
\end{equation}
where $\hat\gamma$ is the maximum likelihood estimate of $\gamma$ and $\text{dim}(\Gamma)$ is the number of parameters to be estimated in the model at hand. The model leading to the lowest BIC value is then retained. Many other BIC-like criteria exist~\cite{vandewalle:tel-00447141} but the underlined idea is unchanged. Contrary to the error rate criteria like~(\ref{eq:error}), it is thus possible to compare models without funding ``non-fundable applicants'' since just the available sample $\gls{T}$ is required. However, computing~(\ref{eq:AIC}) requires to precisely express the model families $\{p_{\gamma}(\gls{y},\gls{z}|\gls{bx})\}_{\gamma \in \Gamma}$ which compete.

\section{Rational reinterpretation of {reject inference} methods} \label{sec:methods_reject}

\subsection{The reject inference challenge} \label{subsec:challenge}

As discussed in the previous section, a regular way to use the whole observed sample $\gls{T}$ in the estimation process implies some challenging modelling and assumption steps. A method using the whole sample $\gls{T}$ is traditionally called a {reject inference} method since it uses not only financed applicants (sample $\gls{Tf}$) but also not financed, or rejected, applicants (sample $\gls{Tnf}$).
Since modelling the financing mechanism $p(\gls{z} | \gls{bx}, \gls{y})$ is sometimes a too heavy task, such methods propose alternatively to use the whole sample $\gls{T}$ in a more empirical manner. However, this is somehow a risky strategy since we have also seen in the previous section that validating methods with error rate like criteria is not possible through the standard \textit{Credit Scoring} process. As a result, some strategies are proposed to perform a ``good'' \gls{score} function estimation without possibility to access their real performance.

However, most of the proposed reject inference strategies may make some hidden assumptions on the modelling process. Our challenge is to reveal as far as possible such hidden assumptions to then discuss their realism,  
failing to be able to compare them by the model selection principle.

\subsection{Strategy 1: ignoring not financed clients} \label{subsec:strat1}

\subsubsection{Definition}
The simplest reject inference strategy is to ignore not financed clients for estimating $\gls{bth}$. Thus it consists to estimate $\gls{bth}$ by maximizing the log-likelihood $\ell(\gls{bth};\gls{Tf})$.

\subsubsection{Missing data reformulation}
In fact, this strategy is equivalent to using the whole sample $\gls{T}$ (financed and not financed applicants) under both the \gls{mar} and ignorable assumptions. See the related explanation in Section~\ref{sec:mechanisms} and~\cite{zadrozny2004learning}. Consequently, this strategy is truly a particular ``reject inference'' strategy although it does not seem to be.

\subsubsection{Estimate property}
By noting $\hat{\gls{bth}}_\text{f}$ and $\hat{\gls{bth}}$ respectively the maximum likelihood estimates of $\ell(\gls{bth};\gls{Tf})$ and $\ell(\gls{bth};\mathcal{T}_{\text{c}})$ provided we know $\gls{y}_i$ for $i \in \gls{NF}$, classical maximum likelihood properties~\cite{10.2307/1912526,zadrozny2004learning} yield under a well-specified model hypothesis (there exists $\gls{bthstar}$ s.t.\ $p(\gls{y} | \gls{bx}) = p_{\gls{bthstar}}(\gls{y} | \gls{bx})$ for all $(\gls{bx},\gls{y})$) and a \gls{mar} missingness mechanism that $\hat{\gls{bth}} \approx \hat{\gls{bth}}_\text{f}$ for large-enough samples $\gls{Tf}$ and $\gls{Tnf}$.



\subsection{Strategy 2: fuzzy augmentation}

\subsubsection{Definition}
This strategy can be found in~\cite{economix} and is developed in depth in Appendix~\ref{fuzzy}. It corresponds to an algorithm which is starting with $\hat{\gls{bth}}^{(0)} = \hat{\gls{bth}}_\text{f}$ (see previous section). Then, all $(\gls{y}_i)^{(1)}_{i \in \gls{NF}}$ are imputed by their expected value given by: $\hat{\gls{y}}^{(1)}_i = p_{\hat{\gls{bth}}^{(0)}}(1 | \gls{bx}_i)$ (notice that these imputed values are not in $\{0,1\}$ but in $]0,1[$). The completed log-likelihood $\ell_c(\gls{bth};\mathcal{T}_{\text{c}}^{(1)})$ given in~(\ref{eq:like.c}) with $\mathcal{T}_{\text{c}}^{(1)}=\gls{Tf} \cup (\gls{y}_i)^{(1)}_{i \in \gls{NF}}$ is maximized and yields parameter estimate $\hat{\gls{bth}}^{(1)}$.

\subsubsection{Missing data reformulation}
%There are two possible interpretations of this method's optimized criterion, as found in~\cite{economix} and developed in depth in Appendix~\ref{fuzzy}: first as a completed log-likelihood as underlined in the previous paragraph; second as a generative log-likelihood $\mathbb{E}_{\gls{bby}_{\text{nf}}}\left[\ell(\gls{bth};\gls{bbx},\gls{bby}_{\text{f}},{\gls{bby}}_{\text{nf}}) \middle| \hat{\gls{bth}}_{\text{f}}\right] = \ell(\gls{bth};\gls{bbx}_{\text{f}},\gls{bby}_{\text{f}}) + \sum_{i=n+1}^{n+n'} \sum_{\gls{y}_i=0}^{1} \mathbb{E}\left[\gls{y}_i \middle| \gls{bx}_i,\hat{\gls{bth}}_{\text{f}}\right] p_{\gls{bth}}(\gls{y}_i|\gls{bx}_i)$. In a way, it is a one-step \gls{em}-algorithm, as presented in Section~\ref{sec:EM}, without taking into account the financing mechanism $p(\gls{z} | \gls{bx},\gls{y})$.

Following the notations introduced in Section~\ref{sec:EM}, and recalling that this method does not take into account the financing mechanism $p(\gls{z} | \gls{bx},\gls{y})$, this method is an \gls{em}-algorithm yielding $\hat{\gls{bth}}^{(1)} = \argmax_{\gls{bth}} \mathbb{E}_{\gls{bby}_{\text{nf}}} [\ell_c(\gls{bth};\mathcal{T}_{\text{c}}^{(1)}) | \gls{T},\hat{\gls{bth}}^{(0)}]$. The complete data $\mathcal{T}_{\text{c}}$ can be schematically expressed as:
\[ \mathcal{T}_{\text{c}}^{(1)} = \left(\begin{array}{c}
\tikzmarkin[hor=style green]{el0} \gls{bbx}_{\f} \tikzmarkend{el0} \\
\\
\\
\tikzmarkin[hor=style green]{el-1} \gls{bbx}_{\nf} \tikzmarkend{el-1} \end{array}
\left( \begin{array}{ccc}
%\rowcolor{red!20}
\tikzmarkin[hor=style green]{el1} \; \; x_{1,1} & \cdots & x_{1,d}  \\
 \vdots & \vdots & \vdots \\
 x_{n,1} & \cdots & x_{n,d} \\
 x_{n+1,1} & \cdots & x_{n+1,d}  \\
 \vdots & \vdots & \vdots \\
 x_{n+n',1} & \cdots & x_{n+n',d} \tikzmarkend{el1} \end{array} \right),
 \hspace{1.5cm}
 \begin{array}{c}
\tikzmarkin[hor=style green]{l1} \gls{bby}_{\f} \tikzmarkend{l1}\\
\\
\\
\tikzmarkin[hor=style green]{l2} \gls{bby}_{\nf} \tikzmarkend{l2} \end{array}
\left( \begin{array}{c}
\tikzmarkin[hor=style green]{l3} \; \; y_1 \; \; \; \\
\vdots \\
 y_n \\ 
 \hat{\gls{y}}_{n+1}^{(1)} \\
\vdots \\
\hat{\gls{y}}_{n+n'}^{(1)} \tikzmarkend{l3}\end{array} \right),
\hspace{1.5cm} 
 \begin{array}{c}
\tikzmarkin[hor=style green]{el111} \gls{bbz}_{\text{f}} \tikzmarkend{el111}\\
\\
\\
\tikzmarkin[hor=style green]{el121} \gls{bbz}_{\text{nf}} \tikzmarkend{el121}\end{array}
\left( \begin{array}{c}
\tikzmarkin[hor=style green]{e41} \text{f} \\
\vdots \\
\text{f} \\ 
\text{nf} \\
\vdots \\
\text{nf} \tikzmarkend{e41} \end{array} \right) \right).\]


\subsubsection{Estimate property}
It can be easily shown (Appendix~\ref{fuzzy}) that $\argmax_{\gls{bth}} \ell_c(\gls{bth};\mathcal{T}_{\text{c}}^{(1)}) = \hat{\gls{bth}}_{\f}$ so that this method is similar to the scorecard learnt on the financed clients.

\subsection{Strategy 3: reclassification}

\subsubsection{Definition}
This strategy corresponds to an algorithm which is starting with $\hat{\gls{bth}}^{(0)} = \hat{\gls{bth}}_\text{f}$ (see Section~\ref{subsec:strat1}). Then, all $(\gls{y}_i)^{(1)}_{i \in \gls{NF}}$ are imputed by the {\it maximum a posteriori} (MAP) principle given by: $\hat{\gls{y}}^{(1)}_i = \argmax_{\gls{y} \in \{0,1\}} p_{\hat{\gls{bth}}^{(0)}}(\gls{y} | \gls{bx}_i)$. The completed log-likelihood $\ell_c(\gls{bth};\mathcal{T}_{\text{c}}^{(1)})$ given in~(\ref{eq:like.c}) with $\mathcal{T}_{\text{c}}^{(1)}=\gls{T} \cup (\gls{y}_i)^{(1)}_{i \in \gls{NF}}$ is maximized and yields parameter estimate $\hat{\gls{bth}}^{(1)}$.

Its first variant stops at this value $\hat{\gls{bth}}^{(1)}$. Its second variant iterates until potential convergence of $(\hat{\gls{bth}}^{(s)})$, $s$ designing the iteration number. In practice, this method can be found in~\cite{saporta} under the name ``iterative reclassification'', in~\cite{RI6} under the name ``reclassification'' or under the name ``extrapolation'' in~\cite{banasik}. It is developed in depth in Appendix~\ref{reclassification}.

\subsubsection{Missing data reformulation}
This algorithm is equivalent to the so-called \gls{cem} algorithm where a Classification (or MAP) step is inserted between the Expectation and Maximization steps of an \gls{em} algorithm (described in Section~\ref{sec:EM}).
% It can be found in \cite{RI6}, also sometimes referred to as extrapolation as in \cite{banasik}. 
\gls{cem} aims at maximizing the completed log-likelihood $\ell_c(\gls{bth};\mathcal{T}_{\text{c}})$ over both $\gls{bth}$ and $(\gls{y}_i)_{i \in \gls{NF}}$. Since $\gls{phi}$ is not involved in this process, we first deduce from Section~\ref{sec:mechanisms} that, again, \gls{mar} and ignorable assumptions are present. Then, standard properties of the estimate maximizing the completed likelihood indicate that it is not a consistent estimate of $\gls{bth}$~\cite{celeux1992classification}, contrary to the traditional maximum likelihood one.

\subsubsection{Estimate property}
The \gls{cem} algorithm is known for ``sharpening'' the decision boundary: predicted probabilities are closer to $0$ and $1$ than their true values as can be seen from simulated data from a \gls{mar} mechanism on Figure~\ref{fig:biais_CEM}. The scorecard $\hat{\gls{bth}}_{\text{f}}$ on financed clients (in \textcolor{green}{green}) is asymptotically consistent as was emphasized in Section~\ref{subsec:strat1} while the reclassified scorecard (in \textcolor{red}{red}) is biased even asymptotically.

\begin{figure}[ht]
\center \includegraphics[width=\textwidth]{figures/chapitre2/CEM_bias.png}
\caption{In the context of a probabilistic classifier, it is known that the CEM algorithm employed implicitly by the Reclassification method amounts to a bigger bias in terms of \gls{lr} parameters, but a ``sharper'' decision boundary.}
\label{fig:biais_CEM}
\end{figure}


\subsection{Strategy 4: augmentation} \label{subsec:augmentation}

\subsubsection{Definition}
Augmentation can be found in \cite{RI6}. It is also documented as a ``Re-Weighting method'' in \cite{saporta,banasik,economix} and is described in Appendix~\ref{augmentation}. This technique is directly influenced by the Importance Sampling~\cite{zadrozny2004learning} literature because intuitively, as for all selection mechanism such as survey respondents, observations should be weighted according to their probability of being in the sample w.r.t.\ the whole population, i.e.\ by $p(\gls{z} | \gls{bx},\gls{y})$. By assuming implicitly a \gls{mar} missingness mechanism, as emphasized in Section~\ref{sec:mechanisms}, we get $p(\gls{z} | \gls{bx}, \gls{y}) = p(\gls{z} | \gls{bx})$. 

For \textit{Credit Scoring} practitioner, the estimate of interest is the \gls{mle} of $\ell(\gls{bth};\gls{T} \cup \gls{bby}_{\nf})$, which cannot be computed since we don't know $\gls{bby}_{\nf}$. However, in Section~\ref{subsec:gradient}, we derived the likelihood from the $\gls{KL}$ divergence by focusing on $\mathbb{E}_{\gls{bX},\gls{Y}} [\ln[p_{\gls{bth}}(\gls{Y}|\gls{bX})]]$. By noticing that $p(\gls{bx}) = \dfrac{p(\gls{bx} | \f)}{p(\f | \gls{bx})} p(\f) $ and by assuming a \gls{mar} missingness mechanism, we get:
\[\mathbb{E}_{\gls{bX},\gls{Y}} [\ln[p_{\gls{bth}}(\gls{Y}|\gls{bX})]] = p(\f) \sum_{\gls{y}=0}^1 \int_{\mathcal{X}} \dfrac{\ln p_{\gls{bth}}(\gls{y} | \gls{bx})}{p(\f | \gls{bx})} p(\gls{y} | \gls{bx}) p(\gls{bx} | \f) d\gls{bx} \approx_{n \to \infty} \dfrac{p(\f)}{n} \sum_{i \in \gls{F}} \dfrac{1}{p(\f | \gls{bx}_i)} \ln p_{\gls{bth}}(\gls{y}_i | \gls{bx}_i).\]
Consequently, had we access to $p(\f | \gls{bx})$, the parameter maximizing the above mentioned likelihood would asymptotically be equal to the one on the through-the-door population, had we access to $\gls{bby}_\nf$. However, $p(\f | \gls{bx})$ must be estimated by the practitioner's method of choice, which will come with its bias and variance.

This method proposes to bin observations in $\gls{T}$ in, say, 10 \textit{equal-length} intervals of the \gls{score} given by $p_{\hat{\gls{bth}}_\text{f}}(1 | \gls{bx})$ and estimate $p(\gls{z} | \gls{bx})$ as the proportion of financed clients in each of these bins. The inverse of this estimate is then used to weight financed clients in $\gls{Tf}$ and retrain the model.

\subsubsection{Missing data reformulation}
The method aims at correcting for the selection procedure yielding the training data $\gls{Tf}$ in the \gls{mar} case. As was argued in Section~\ref{subsec:strat1}, if the model is well-specified, such a procedure is superfluous as the estimated parameter $\hat{\gls{bth}}_{\f}$ is consistent. In the misspecified case however, it is theoretically justified as will be developed in the next paragraph. However, it is unclear if this apparent benefit is not offset by the added estimation procedure (which comes with its bias / variance trade-off).

\subsubsection{Estimate property}
The Importance Sampling paradigm requires $p(\f | \gls{bx}) > 0$ for all $\gls{bx}$ which is clearly not the case here: for example, jobless people are never financed.

\subsection{Strategy 5: twins}

\subsubsection{Definition}
This {reject inference} method is documented internally at \gls{cacf} and in Appendix~\ref{Twins}; it consists in combining two scorecards: one predicting $\gls{y}$ learnt on financed clients (denoted by $\hat{\gls{bth}}_{\text{f}}$ as previously), the other predicting $\gls{z}$ learnt on all applicants, before learning the final scorecard using the predictions made by both scorecards on financed clients.

\subsubsection{Missing data reformulation}
The method aims at re-injecting information about the financing mechanism in the \gls{mar} missingness mechanism by estimating $\hat{\gls{phi}}$ as a \gls{lr} on all applicants, calculating \gls{score}s $(1,\gls{bx})' \hat{\gls{bth}}_\f$ and $(1,\gls{bx})' \hat{\gls{phi}}$ and use these as two continuous features in a third \gls{lr} predicting again the repayment feature $\gls{y}$.

\subsubsection{Estimate property}
It can be easily shown (Appendix~\ref{Twins}) that
% $\argmax_{\gls{bth}} \ell(\gls{bth};(\bm{1},\gls{bbx})' \hat{\gls{phi}}, (\bm{1},\gls{bbx})' \hat{\gls{bth}}_\f, \gls{bby}) = \hat{\theta}^{\text{f}}$ so that 
this method is similar to the scorecard learnt on the financed clients.

\subsection{Strategy 6: parcelling} \label{subsec:parcel}

\subsubsection{Definition}
The parcelling method can be found in~\cite{saporta,banasik,RI6}. It is also described in Appendix~\ref{Parceling}. This method aims to correct the log-likelihood estimation in the \gls{mnar} case by making further assumptions on $p(\gls{y} | \gls{bx}, \gls{z})$. It is a little deviation from the fuzzy augmentation method in a \gls{mnar} setting: we start with $\hat{\gls{bth}}^{(0)} = \hat{\gls{bth}}_\f$ and the practitioner arbitrarily decides to discretize the subsequent range of scores $(p_{\hat{\gls{bth}}^{(0)} }(\gls{y}_i | \gls{bx}_i))_1^{n+n'}$ into, say, $K$ scorebands $B_1, \dots, B_K$ and ``prudence factors'' $\bm{\epsilon} = (\epsilon_1, \dots, \epsilon_K)$ generally such that $1 < \epsilon_1 < \dots < \epsilon_K$ (non-financed low refunding probability clients are considered way riskier, all other things equal, than their financed counterparts). The method is thereafter strictly equivalent to fuzzy reclassification: a new \gls{lr} parameter is deduced from maximizing the expected complete log-likelihood as follows:
\[ \hat{\gls{bth}}^{(1)} = \argmax_{\gls{bth}} \mathbb{E}_{\gls{bby}_{\text{nf}}} [\ell_c(\gls{bth};\mathcal{T}_{\text{c}}) | \gls{T},\hat{\gls{bth}}^{(0)}, \bm{\epsilon}] = \ell(\gls{bth};\gls{Tf}) + \sum_{i' = n + 1}^{n+n'} \epsilon_i p_{\hat{\gls{bth}}^{(0)}}(\gls{y}_i | \gls{bx}_i) \ln p_{\gls{bth}}(\gls{y}_i | \gls{bx}_i), \]
where $\epsilon_i = \sum_{k=1}^K \epsilon_k \mathds{1}(p_{\hat{\gls{bth}}^{(0)}}(\gls{y}_i | \gls{bx}_i) \in B_k)$ is simply the prudence factor of each individual, depending on their scoreband, decided by the practitioner.

\subsubsection{Missing data reformulation}
By considering not-financed clients as riskier than financed clients with the same level of \gls{score}, \textit{i.e.}\ $p(\gls{y} | \gls{bx}, \nf) > p(\gls{y} | \gls{bx}, \f)$, it is implicitly assumed that manual operators (see Figure~\ref{fig:figure1}) have access to additional information, say $\tilde{\gls{bx}}$ such as supporting documents, that influence the outcome $\gls{y}$ even when $\gls{bx}$ is accounted for. In this setting, rejected and accepted clients with the same \gls{score} differ only by $\tilde{\gls{bx}}$, to which we do not have access and is accounted for ``quantitatively'' in a user-defined prudence factor $\bm{\epsilon}$ stating that rejected clients would have been riskier than accepted ones.

\subsubsection{Estimate property}
%A completed dataset $\mathcal{T}_{\text{c}}$ is derived by making bins of observations in $\gls{T}$ in, say, 10 \textit{equal-length} intervals of the \gls{score} given by $p_{\hat{\gls{bth}}_\text{f}}(1 | \gls{bx})$ and estimating $p(\gls{y} | \gls{bx}, \nf)$ as the mean of $\gls{Y} | \gls{bx}, \f$ in each bin multiplied by a prudence factor encompassing the practitioner's \textit{belief} about the effectiveness of the operators' rejections.

The prudence factor encompasses the practitioner's \textit{belief} about the effectiveness of the operators' rejections. It cannot be estimated from the data nor tested and is consequently a matter of unverifiable expert knowledge.

%\bigskip
%
%Since it is possible to conclude about these reject inference methods analytically (see next section), simulations and numerical experiments on real data have been deferred to the Appendices~\ref{subsec:app_reject_sim}, \ref{subsec:app_reject_real} and~\ref{subsec:app_reject_real_method} where reject inference methods are applied to simulated data, real data and compared to other predictive methods respectively.

\section{Numerical experiments} \label{sec:num_exp_reject}

Appendix~\ref{subsec:app_reject_sim} shows all reject inference methods developed here applied to simulated data from a multivariate Gaussian distribution for each class. A \gls{lr} is learnt on all drawn observations, yielding $\hat{\gls{bth}} = \argmax_{\gls{bth}} \ell(\gls{bth}; \mathcal{T}_{\text{c}})$ where $\gls{bby}_{\nf}$ is known. We simulate not-financed clients by progressively ``rejecting'' observations such that $p_{\hat{\gls{bth}}}(1 | \gls{bx}_i) < \epsilon$ with a varying threshold $\epsilon$. This corresponds to a well-specified \gls{lr} and a MAR missingness mechanism. It comes as no surprise that no reject inference method performs better than standard \gls{lr} on financed clients (strategy 1). Since the data generating mechanism is Gaussian homoscedastic, we also resorted to semi-supervised linear discriminant analysis (LDA). This is straightforward with the Rmixmod \textsf{R} package~\cite{lebret2014rmixmod}: the membership of unlabeled observations is assessed by the \gls{em} algorithm. Although the decision boundaries of these two models is asymptotically equivalent, by making further assumptions, LDA suffers from less variance even asymptotically~\cite{efron1975efficiency}. With less financed clients, its advantage becomes clearer since it benefits from information on non-financed clients to evaluate $p(\gls{bx})$. This model was tested on real data but since the normality assumption does not hold, it shows poor results.

Here we focus on reject inference methods based on \gls{lr} applied to various \gls{cacf} datasets: Electronics loans, Sports goods and Standard loans. They contain $n=180{,}000$, $n=35{,}000$ and $n=28{,}000$ respectively and $d=5$, $d=8$ and $d=6$ categorical features with $3$ to $10$ levels per feature. The Electronics dataset consists in one year of financed clients through a partner of \gls{cacf} that mainly sells electronics goods. The Sports dataset consists in one year of financed clients through a partner of \gls{cacf} that sells all kinds of sports goods. The Standard consists in one year of financed clients stemming directly from sofinco.fr. The acceptance / rejection mechanism $p_{\gls{phi}}(\gls{z} | \gls{bx})$ is the existing scorecard and we simulate rejected applicants by progressively increasing the \gls{cut} (the preceding threshold $\epsilon$) of the existing scorecard.

The results in terms of Gini index are reported in Figure~\ref{fig:darty_reject},~\ref{fig:decathlon_reject} and~\ref{fig:M3_reject} respectively. All methods perform relatively similarly and suffer from a big performance drop once the acceptance rate is below 50 \% (at which point there are very few ``bad borrower'' events - $y = 0$). If we were to report 95 \% confidence intervals around the Gini indices, we would get insignificant predictive performances, which confirms our theoretical findings.

Other predictive models are compared to \gls{lr} on the real datasets presented here in Appendix~\ref{subsec:app_reject_real_method}. These experiments confirm that ``global'' methods in the sense of~\cite{zadrozny2004learning} (which explicitly or implicitly estimate $p(\gls{bx})$ to access $p(\gls{y} | \gls{bx})$ like LDA without unlabelled observations) degrade rapidly when the proportion of financed clients decreases.

%(beware: the colors / symbols do not correspond to the same methods in each graph). 
% The acceptance / rejection mechanism $p_{\gls{phi}}(\gls{z} | \gls{bx})$ is again simulated using a \gls{lr}. Two things are striking in these figures: first, all methods perform relatively similarly and suffer a big performance drop once the acceptance rate is below 50 \% (at which point there are very few ``bad borrower'' events - $y = 0$); second, the multivariate generative model that performed best on simulated data in the Appendix~\ref{subsec:app_reject_sim} (since it is the well-specified model) performs worst with real data, since it makes more hypotheses and is thus less flexible. Two other sets of experiments were carried out: first, on simulated data from a well-specified model, \gls{mar} missingness mechanism. Following our findings in the preceding section, ignoring not financed clients yields the best results for semi-parametric models as can be seen in Appendix~\ref{subsec:app_reject_sim}. Second, other predictive models are compared to \gls{lr} on the real datasets presented here in Appendix~\ref{subsec:app_reject_real_method}. These experiments confirm that ``global'' methods in the sense of~\cite{zadrozny2004learning} degrade rapidly when the proportion of financed clients decreases.

\begin{figure}[!ht]
\centering \resizebox{.8\textwidth}{!}{\input{R_CODE_FIGURES/chapitre2/CODE_REINTEGRATION/electronics.tex}}
\caption{Performance resulting from the use of reject inference methods in terms of Gini on an Electronics loans dataset from \gls{cacf}.}
\label{fig:darty_reject}
\end{figure}

\begin{figure}[!ht]
\centering \resizebox{.8\textwidth}{!}{\input{R_CODE_FIGURES/chapitre2/CODE_REINTEGRATION/sports.tex}}
\caption{Performance resulting from the use of reject inference methods in terms of Gini on a Sports goods loans dataset from \gls{cacf}.}
\label{fig:decathlon_reject}
\end{figure}

\begin{figure}[!ht]
\centering \resizebox{.8\textwidth}{!}{\input{R_CODE_FIGURES/chapitre2/CODE_REINTEGRATION/standard.tex}}
\caption{Performance resulting from the use of Reject Inference methods in terms of Gini on a Standard loans dataset from \gls{cacf}.}
\label{fig:M3_reject}
\end{figure}

\section{Discussion: choosing the right model} \label{sec:conclusion_reject}

\subsection{Sticking with the financed clients model}

Constructing scorecards by using a \gls{lr} on financed clients is a trade-off: on the one hand, it is implicitly assumed that it is well-specified, and that the missingness mechanism governing the observation of $\gls{Y}$ is \gls{mar}. In other words, we suppose $p(\gls{y} | \gls{bx}) = p_{\gls{bthstar}}(\gls{y} | \gls{bx}, \f)$. On the other hand, these assumptions, which seem strong at first hand, cannot really be relaxed: first, the use of \gls{lr} is a requirement from the financial institution.
%, such that the straightforward \gls{em} algorithm of section~\ref{sec:EM} cannot be carried out. 
Second, the comparison of models cannot be performed using standard techniques since $\gls{bby}_{\nf}$ is missing (section~\ref{subsec:model_selection}). Third, strategies 4 and 6 that tackle the misspecified model and \gls{mnar} settings respectively require additional estimation procedures that, supplemental to their estimation bias and variance (section~\ref{subsec:gradient}), take time from the practitioner's perspective and are rather subjective (see sections~\ref{subsec:augmentation} and~\ref{subsec:parcel}), which is not ideal in the banking industry since there are auditing processes and model validation teams that might question these practices.

\subsection{\gls{mcar} through a Control Group}

Another simple solution often stated in the literature would be to keep a small portion of the population where applicants are not filtered: everyone gets accepted. This so-called \textit{Control Group} would constitute the learning and test sets for all scorecard developments.

Although theoretically perfect, this solution faces a major drawback: it is costly, as many more loans will default. To construct the scorecard, a lot of data is required, so the minimum size of the \textit{Control Group} is equivalent to a much bigger loss than the amount a bank would accept to lose to get a few more Gini points.

\subsection{Keep several models in production: ``champion challengers''}

Several scorecards could also be developed, e.g. one using each {reject inference} technique. Each application is randomly scored by one of these scorecards. As time goes by, we would be able to put more weight on the most performing scorecard(s) and progressively less on the least performing one(s): this is the field of Reinforcement Learning~\cite{Sutton1998}.

The major drawback of this method, although its cost is very limited unlike the \textit{Control Group}, is that it is very time-consuming for the credit modeller who has to develop several scorecards, for the IT who has to put them all into production, for the auditing process and for the regulatory institutions.

\bigskip

For years, the necessity of {reject inference} at \gls{cacf} and other institutions (as it seems from the large literature coverage this research area has had) has been a question of personal belief. Moreover, there even exists contradictory findings in this area.

By formalizing the {reject inference} problem in Section~\ref{sec:criteres}, we were able to pinpoint in which cases the current scorecard construction methodology, using only financed clients' data, could be unsatisfactory: under a \gls{mnar} missingness mechanism and/or a misspecified model. We also defined criteria to reinterpret existing {reject inference} methods and assess their performance in Section~\ref{subsec:model_selection}. We concluded that no current {reject inference} method could enhance the current scorecard construction methodology: only the augmentation method (strategy 4) and the parcelling method (strategy 6) had theoretical justifications but introduce other estimation procedures. Additionally, they cannot be compared through classical model selection tools (Section~\ref{subsec:model_selection}).

We confirmed numerically these findings in the Appendices: given a true model and the \gls{mar} assumption, no \gls{lr}-based {reject inference} method performed best than the current method. In the misspecified model case, the augmentation method seemed promising but it introduces a model that also comes with its bias and variance resulting in very close performances compared with the current method. With real data provided by \gls{cacf}, we showed that all methods gave very similar results: the ``best'' method (by the Gini index) was highly dependent on the data and/or the proportion of unlabelled observations. Last but not least, in practice such a benchmark would not be tractable as $\gls{bby}_{\nf}$ is missing. In light of those limitations, adding to the fact that implementing those methods is a non-negligible time-consuming task, we recommend credit modellers to work only with financed loans' data unless there is significant information available on either rejected applicants ($\gls{bby}_{\nf}$ - credit bureau information for example, which does not apply to France) or on the acceptance mechanism $\gls{phi}$ in the \gls{mnar} setting. On a side note, it must be emphasized that this work only applies to \gls{lr} and can be extended to all ``local'' models per the terminology introduced in~\cite{zadrozny2004learning}. For ``global'' models, \textit{e.g.} decision trees, it can be shown that they are biased even in the \gls{mar} and well-specified settings, thus requiring \textit{ad hoc} reject inference techniques such as an adaptation of the augmentation method (strategy 4 - see~\cite{zadrozny2004learning}).

\printbibliography[heading=subbibliography, title=References of Chapter 2]