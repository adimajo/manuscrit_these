\chapter{Tree-structure segmentation for logistic regression} \label{chap6}

\selectlanguage{english}

\epigraph{All religions, arts and sciences are branches of the same tree. All these aspirations are directed toward ennobling man's life, lifting it from the sphere of mere physical existence and leading the individual towards freedom.}{Albert Einstein, ``Moral Decay'', 1937.}

\minitoc

\bigskip

In Chapter~\ref{chap1}, it was argued in Section~\ref{subsec:segmentation} that, what is referred to as ``segmentation'' in the \textit{Credit Scoring} industry, could be a straightforward solution to deal with missing values and outliers that are quite common problems in \textit{Credit Scoring}. This means we learn ``expert'' \gls{lr} models on separate ``segments'' of clients arranged in a tree. Its more theoretical justification is similar to that of quantization, sketched in Section~\ref{sec:bias_variance_quant}, which is to achieve a good bias-variance trade-off of the predictive task. This goal was embedded explicitly in the proposed quantization algorithm. Here again, the resulting segmentation and scorecards therein can be viewed as a single model for the whole population. In the next section, we give some industrial context to the problem which is followed in Section~\ref{sec:literature} by a literature review. Section~\ref{sec:model_selec_tree} reinterprets this problem, as advertised, as a model selection problem for which a specific approach is designed in subsequent sections.


\section{Introduction}

\subsection{Context} \label{subsec:context}

As was emphasized in all previous chapters, \gls{lr} is the building block of a scorecard predicting the creditworthiness of an applicant and partly automating the acceptance / rejection mechanism. However, estimating \gls{lr} coefficients means that training data $(\gls{bbx},\gls{bby})$ is available. This is not the case when a new product, \textit{e.g.}\ smartphone leasing, is added to the acceptance system. On a practical note, some other previously learnt scorecard may not be applicable on this new market because the same information is not asked to applicants, \textit{e.g.}\ marital status, because given the low amounts at stake, it was decided to collect the fewest data possible, to make the process as simple and quick as possible. On a more theoretical note, it is probable that applicants to smartphone leasing are not stemming from the same data generating mechanism $\gls{bX},\gls{Y} \sim p$ as any other previous applicants (\textit{i.e.}\ on other markets). Put it another way, the possibility of having several \gls{lr} scorecards on sub-populations of the total portfolio allows to have more flexibility, and thus it potentially reduces model bias discussed in Chapter~\ref{chap1}.

For these reasons, several industries, among which \textit{Credit Scoring}, rely on several ``weak learners'' such as \gls{lr}, arranged in a tree. Such decision process is illustrated on Figure~\ref{fig:arbre}. This tree structure and the vocabulary of ``weak learners'' would indicate a use-case of Mixtures of Experts~\cite{jordan1994hierarchical} or aggregation / ensemble methods~\cite{opitz1999popular} respectively. However, these fuzzy methods imply that all applicants are scored by all scorecards, which is obviously neither desirable (for interpretation purposes) nor feasible (since available features differ).

The next section illustrates how such a structure is achieved using \gls{cacf}'s in-house practices.

\tikzstyle{level 1}=[level distance=2.2cm, sibling distance=7cm]
\tikzstyle{level 2}=[level distance=2cm, sibling distance=5cm]
\tikzstyle{level 3}=[level distance=2cm, sibling distance=3cm]

\begin{figure}
\resizebox{\textwidth}{!}{
\centering
\begin{tikzpicture}
  [
    sibling distance        = 15em,
    level distance          = 5em,
    edge from parent/.style = {draw, -latex},
    every node/.style       = {font=\footnotesize},
    sloped
  ]
  \node [root] {\textcolor{black}{Applicants}}
    child { node [dummy] {}
      child { node [dummy] {}
        child { node [env] {\textcolor{black}{$p_{\gls{bth}^1}(y|\q^1(\gls{bx}),\bdelta^1)$ \hspace*{-0.4cm}}}
          edge from parent node [below] {Renters} }
        child { node [env] {\textcolor{black}{$p_{\gls{bth}^2}(y|\q^2(\gls{bx}),\bdelta^2)$ \hspace*{-0.4cm}}}
          edge from parent node [above] {Salaried} }
        child { node [env] {\textcolor{black}{$p_{\gls{bth}^3}(y|\q^3(\gls{bx}),\bdelta^3)$ \hspace*{-0.4cm}}}
                edge from parent node [above] {Others} }
        edge from parent node [above] {Revolving} }
      child { node [env] {\textcolor{black}{$p_{\gls{bth}^4}(y|\q^4(\gls{bx}),\bdelta^4)$ \hspace*{-0.4cm}}}
              edge from parent node [above, align=center]
                {Standard loan} }
              edge from parent node [above] {Home Appliances} }
    child { node [dummy] {}
      child { node [dummy] {}
        child { node [env] {\textcolor{black}{$p_{\gls{bth}^5}(y|\q^5(\gls{bx}),\bdelta^5)$ \hspace*{-0.4cm}}}
          edge from parent node [above] {Leasing} }
        child { node [env] {\textcolor{black}{$p_{\gls{bth}^6}(y|\q^6(\gls{bx}),\bdelta^6)$ \hspace*{-0.4cm}}}
                edge from parent node [above] {Standard loan} }
        edge from parent node [above] {Fiat} }
      child { node [env] {\textcolor{black}{$p_{\gls{bth}^7}(y|\q^7(\gls{bx}),\bdelta^7)$ \hspace*{-0.4cm}}}
              edge from parent node [above, align=center]
                {Kawasaki} }
              edge from parent node [above] {Automobile} };
\end{tikzpicture}
}
\caption{Simplified cartography of the application scorecards.}
\label{fig:arbre}
\end{figure}




\subsection{In-house \textit{ad hoc} practice} \label{subsec:adhoc}

\textit{Credit Scoring} practitioners are often asked by the management to ``locally'' study the decision process displayed on Figure~\ref{fig:arbre} by \textit{e.g.}\ merging branches (Standard loans and Leasing for Fiat) or conversely to separate sub-populations by splitting a leaf (Kawasaki into Standard loans and Leasing). To do so, they resort to simple unsupervised generative clustering techniques, such as \gls{pca} and its refinements on categorical or mixed data (\gls{mca} or \gls{famd} resp.) which are described hereafter.

\paragraph{\glsfirst{pca}}

The goal of \gls{pca}~\cite{pages2014multiple} is to represent observations graphically in a way that exhibits most efficiently their similitude and differences by combining input features in so-called orthogonal ``principal components'' $\bm{u} = (\bm{u}_1,\dots,\bm{u}_d)$ such that the inertia of each axis $j$ (the variance of $\gls{bx}' \bm{u}_j$) is maximized. It can be shown that it is equivalent to seeking the ordering of the eigenvalues $\bm{\lambda} = (\lambda_1,\dots,\lambda_d)$ of the covariance matrix $\Sigma = \gls{bbx}'\gls{bbx}$. The explained variance of each axis $j$ is given by $\frac{\lambda_j}{\sum_{j'=1}^d \lambda_j'}$. Classically, only the two first axes $(\bm{u}_1,\bm{u}_2)$ (after reordering from largest to lowest explained variance) are used. The composition of theses axes in the original features $\gls{x}_j$ is often represented first, to see if groups of features can be formed which would define the subsequent segments. \gls{pca} has been applied to the Automobile dataset from \gls{cacf} ($n = 50{,}000$, $d = 25$ among which $18$ continuous and $7$ categorical features which number of levels go from $5$ - family status - to $100$ - brand of the vehicle and $200{,}000$ missing entries) on Figure~\ref{fig:pca}, where the aforementioned principal components' composition is displayed on Figure~\ref{fig:pca1}: interestingly, the first axis is dominated by car and loan characteristics such as the vehicle's price (``APPORT'', ``MCDE'', ``CREDAC''), its fiscal and mechanical characteristics (``CVFISC'', ``POIDSVH'', ``CYLVH'') while the second axis is composed of clients' characteristics such as their age (``anc\_DNAISS'', ``anc\_DNACJ''), their number of children (``NBENF''), their job stability (``anc\_AMEMBC''). Note that a good portion of the total variance is explained by the first axes (22.54 \% and 18.97 \% resp.). The second classical representation is the observations themselves in this new space, which is displayed on Figure~\ref{fig:pca2}: no clear group is distinguishable from the pack. With these two representations, the \textit{Credit Scoring} practitioner decide if, visually, clusters are formed (\textit{i.e.}\ clouds with little intra-class variance) which would be used to build separate scorecards $(p_{\gls{bth}^{1}},p_{\gls{bth}^{2}})$.
%The method is described in Appendix~\ref{app1:pca}.

However, the \textit{Credit Scoring} data is of mixed type and \textit{Credit Scoring} practitioners are used to quantizing the data, as explained thoroughly in Chapter~\ref{chap4}, such that the \gls{mca} algorithm, specific to categorical features, becomes applicable to all features, by using \textit{e.g.\ equal-freq} or $\chi^2$ tests (see Appendix~\ref{app1}).

\begin{figure}[!htb]
{\setlength{\parindent}{0cm}}
\begin{multicols}{2}
\centering
\begin{subfigure}[t]{0.45\textwidth}
\centering
\resizebox{\textwidth}{!}{\input{R_CODE_FIGURES/chapitre6/pca_axes_definition.tex}}
\caption{\label{fig:pca1} The first axis is dominated by features linked to the financed good and characteristics of the loan while the second axis is composed of characteristics of the client and durations.}
\end{subfigure}%
\columnbreak
\hspace*{1cm} \begin{subfigure}[t]{0.45\textwidth}
\centering
\resizebox{\textwidth}{!}{\input{R_CODE_FIGURES/chapitre6/pca_points.tex}}
\caption{\label{fig:pca2} However, no clear segments appear on the new representation of data along these two axes.}
\end{subfigure}
\end{multicols}
\caption{\label{fig:pca} The result of a \gls{pca} applied to continuous features of \gls{cacf} data from the car loan market.}
\end{figure}

\paragraph{\glsfirst{mca}}

In presence of only categorical features (or following quantization), the \gls{mca} algorithm is more appropriate: it extends the \gls{pca} approach to categorical features by using the disjunctive table (dummy / one-hot encoding of $\gls{bbx}$ described in Section~\ref{subsec:apprentissage}). For a thorough introduction to \gls{mca}, see \textit{e.g.}~\cite{lebart1995statistique}. What is most interesting in this method is that both categorical features' levels and observations can be simultaneously displayed on the first principal components axes.
%The method is described in Appendix~\ref{app1:mca}.
This is of high practical interest in \textit{Credit Scoring} because clouds of points are directly characterized by the categorical levels that are displayed nearest, contrary to \gls{pca} where groups correspond to surfaces of equation $\gls{bx} ' \bm{u}_1 + \gls{bx} ' \bm{u}_2 \geq \alpha$ where $\alpha$ encodes the separation boundary of resulting clusters, which would be the edges of our decision system, as on Figure~\ref{fig:arbre}, and make it arguably less interpretable.

When applied to the Automobile dataset, the \gls{mca} algorithm yields Figure~\ref{fig:mca}. As categorical features' levels are dummy encoded, they are all represented separately as in Figure~\ref{fig:mca1}. Unfortunately, as the vehicle's brand takes a lot of levels, this figure is not very informative. A useful trick, apart from grouping levels, is to plot the barycentre of a feature's levels (weighted by the number of observations in each level), as displayed on Figure~\ref{fig:mca2}. Note that a low portion of the total variance is explained by the first axes (1.51 \% and 1.14 \% resp.) since the data, when one-hot encoded, is very high dimensional. As for \gls{pca}, no groups are formed when displaying the (uninformative) equivalent of Figure~\ref{fig:pca1} and no factor level(s) is / are isolated from the others in Figure~\ref{fig:mca1}. A practitioner would conclude the absence of segments on which to build several scorecards.

Nevertheless, a method applicable to mixed data exists as well and could directly be applied to ``raw'' features.

\begin{figure}[!htb]
{\setlength{\parindent}{0cm}}

\begin{multicols}{2}
\centering
\begin{subfigure}[t]{0.45\textwidth}
\centering
\resizebox{\textwidth}{!}{\input{R_CODE_FIGURES/chapitre6/mca_levels.tex}}
\caption{\label{fig:mca1} The factor map contains all levels of all categorical features, making it hard to read. Note that very little of the total variance is explained by the two first axes.}
\end{subfigure}%
\columnbreak
\hspace*{1cm} \begin{subfigure}[t]{0.45\textwidth}
\centering
\resizebox{\textwidth}{!}{\input{R_CODE_FIGURES/chapitre6/mca_features.tex}}
\caption{\label{fig:mca2} The barycenter of a categorical feature is obtained by weighting the contribution of each level in Figure~\ref{fig:famd1}.}
\end{subfigure}
\end{multicols}

\caption{\label{fig:mca} The result of an \gls{mca} applied to categorical data of \gls{cacf} data from the car loan market.}
\end{figure}

\paragraph{\glsfirst{famd}}

The \gls{famd} algorithm~\cite{pages2014multiple} aims at performing both \gls{mca} on categorical features and \gls{pca} on continuous features in a simultaneous fashion. Resulting principal component axes depend on both data types, as can be seen from Figure~\ref{fig:famd}. As on Figures~\ref{fig:pca1} and~\ref{fig:mca1}, categorical features' levels' and continuous features' contributions to the two first principal components can be displayed on Figure~\ref{fig:famd1}, where vehicle brands make it rather hard to read. When switching to the categorical features' levels barycentre representation, as in Figure~\ref{fig:mca2}, interpretation is easier and somewhat similar to the \gls{pca} method (up to a permutation on the first and second components): the first axis contains information about the client, represented by continuous (\textit{e.g.}\ age) and categorical features (\textit{e.g.}\ job category) while the second axis is about the loan and the vehicle (its brand, cost, etc.).
%The method is described in Appendix~\ref{app1:famd}.
This method has the advantage of not requiring the \textit{Credit Scoring} practitioner to preprocess the ``raw'' data by quantizing it, which could have a huge impact on the results of the subsequent method employed, as argued in Chapter~\ref{chap4}. Moreover, the practitioner would fine-tune the quantization of each sub-population which, if fed back to the \gls{mca}, would potentially yield completely different results! As for \gls{pca} and \gls{mca}, the equivalent of Figure~\ref{fig:pca1} (not shown here) for \gls{famd} does not display distinguishable groups of observations. Nevertheless, the luxury car brands are now well separated in Figure~\ref{fig:famd1} from other continuous features and other categorical features' levels which would be interpreted by a \textit{Credit Scoring} practitioner as the need to build a specific scorecard for this market. However, due to the low volumes of applicants and considering that all of them are probably good clients that are all accepted (the score has little relevance for such markets), no segmentation would be performed.

\medskip

It appears clearly that all these methods do not directly optimize a predictive goal such as the one optimized by \gls{lr}. Moreover, the \textit{ad hoc} preprocessing step of quantization might influence the structure of the retained segmentation. 

For numerical experiments of Section~\ref{sec:num_exp}, we will use, among others, the \gls{famd} approach.

\begin{figure}[!htb]
{\setlength{\parindent}{0cm}}

\begin{multicols}{2}
\centering
\begin{subfigure}[t]{0.45\textwidth}
\centering
\resizebox{\textwidth}{!}{\input{R_CODE_FIGURES/chapitre6/famd_levels.tex}}
\caption{\label{fig:famd1} Analogous of Figure~\ref{fig:mca1}, categorical features' levels involved in the \gls{famd} can be plotted but not much can be drawn from the graph.}
\end{subfigure}%
\columnbreak
\hspace*{1cm} \begin{subfigure}[t]{0.45\textwidth}
\centering
\resizebox{\textwidth}{!}{\input{R_CODE_FIGURES/chapitre6/famd_features.tex}}
\caption{\label{fig:famd2} Categorical features can be represented as the barycenter of their levels as on Figure~\ref{fig:mca2} alongside the contribution of continuous features. The interpretation of the axes is switched: the first one is composed of clients' characteristics, \textit{e.g.}\ age and job, while the second relates to the financed good and the characteristics of the loan, \textit{e.g.}\ its brand or the down payment.}
\end{subfigure}
\end{multicols}

\caption{\label{fig:famd} The result of a \gls{famd} applied to categorical data of \gls{cacf} data from the car loan market.}
\end{figure}

\subsection{These practices can fail} \label{subsec:fail}

Of course, like all \textit{ad hoc} methods that rely on ``two-stages'' procedures (find segments using an \gls{mca} algorithm and learn separate \gls{lr} scorecards on them) which do not share a common objective, the aforementioned in-house practice can fail. \textit{Credit Scoring} practitioners are probably aware that their methods are not bullet-proof, but like most industries, unless provided to them with easily usable software replacing these methods, these practices remain.

This chapter has no intent in filling that gap as was ambitioned in Chapters~\ref{chap4} and~\ref{chap5} but rather to give insights on more elaborate, readily usable methods that will be covered in Section~\ref{sec:literature} and to propose a few ideas for future research. That is why, in the present, we show two data generating mechanisms where current in-house methods fail. In Section~\ref{sec:model_selec_tree}, we will propose an \gls{sem} algorithm that shares similitude with the one proposed for quantization in Section~\ref{sec:sem} that performs well where current methods fail.

The first of these failing situations is when the \gls{pdf} of covariates (suppose for simplicity that all of them are continuous) $p(\gls{bx})$ is multi-modal as on Figure~\ref{fig:xdiff} where we distinguish the lower, middle and upper-classes of respective low, average and high wages and indebtedness. An unsupervised generative approach like \gls{pca} would urge the practitioner to construct 3 scorecards (one for each of the aforementioned classes). However, displaying $y$ as \textcolor{red}{red} (resp.\ \textcolor{green}{green}) for bad borrowers (resp.\ good borrowers), we can see that perfect separation can be achieved: it depends solely on the indebtedness level (the ratio of wages over indebtedness). Thus, the resulting scorecards would be asymptotically the same, but they use three times more parameters! In a finite sample setting, and following remarks in Chapters~\ref{chap1}, \ref{chap4} and~\ref{chap5} on model bias and model selection consistency, it will imply lower performance since each of these coefficients have three times less samples to train on, which amounts to increasing the variance by the same factor. On a practical note, one could argue that it reduces interpretability by adding an avoidable complexity to the decision system. This particular data generating mechanism is revisited in the experiments of Section~\ref{subsec:num_sim}.


\begin{figure}
\centering
\begin{tikzpicture}


\begin{axis}[xtick=\empty, ytick=\empty, xlabel={\LARGE Revenus}, ylabel={\LARGE Endettement}]
\myGlobalTransformation{0}{0};

% pauvres
\addplot [green, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand+x};
\addplot [red, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand+x+2.5};

%moyens
\addplot [green, only marks, mark=*, samples=300, mark size=0.75,domain=2:6]{rand+x};
\addplot [red, only marks, mark=*, samples=300, mark size=0.75,domain=2:6]{rand+x+2.5};

%riches
\addplot [green, only marks, mark=*, samples=300, mark size=0.75,domain=7:11]{rand+x};
\addplot [red, only marks, mark=*, samples=300, mark size=0.75,domain=7:11]{rand+x+2.5};

%frontière
\addplot [black, very thick, domain=-3:11] {x+1.25};
\end{axis}


\end{tikzpicture}
\caption{\label{fig:xdiff} Multi-modal wages and indebtedness data generating mechanism with $y = \{0,1\}$ classes displayed in \textcolor{red}{red} and \textcolor{green}{green} respectively.}
\end{figure}

The second failing situation is the counterpart of the first tailored data generating mechanism. This time, suppose the covariates wages and indebtedness are uniformly sampled. Suppose there is a third categorical feature ``wages source'' which is drawn uniformly from three levels: renters, salaried workers and self-employed. One could argue that renters' risk level do not depend on their indebtedness, which is typically low (and a higher one is a major red flag), salaried workers' risk level is positively correlated with their indebtedness ratio as was the case for the first introductory example (see Figure~\ref{fig:xdiff}) and self-employed people's risk level is negatively correlated with this indebtedness ratio (say, the higher their personal engagement, the higher the chances of success of their business). This example data generating mechanism is illustrated on Figure~\ref{fig:ydiff}. In this situation, and contrary to the first example, an unsupervised generative clustering algorithm like \gls{pca} would not partition the data and the \textit{Credit Scoring} practitioner would construct only one scorecard. This scorecard would have high model bias since it is too simple to accommodate for the variety of the data generating mechanism and consequently perform poorly. This particular data generating mechanism is also revisited in the Experiments of Section~\ref{subsec:num_sim}.


\begin{figure}
\centering
\begin{tikzpicture}


\begin{axis}[xtick=\empty, ytick=\empty, xlabel={\LARGE Revenus}, ylabel={\LARGE Endettement},domain=-3:1, enlargelimits=false,ymin=-3,ymax=6]
\myGlobalTransformationbis{0}{0};

% techniciens
\addplot [green, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand};
\addplot [green, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand-2};
\addplot [green, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand-4};
\addplot [red, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand+2.5};
\addplot [red, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand+4.5};
\addplot [red, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand+6.5};

%frontière
\addplot [black, very thick, domain=-3:1] {1.25};
\end{axis}





\begin{axis}[xtick=\empty, ytick=\empty, xlabel={\LARGE Revenus}, ylabel={\LARGE Endettement},domain=-3:1, enlargelimits=false,ymin=-3,ymax=6]
\myGlobalTransformationbis{0}{3};

% cadres
\addplot [green, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand-x};
\addplot [green, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand-x-2};
\addplot [green, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand-x-4};
\addplot [red, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand-x+2.5};
\addplot [red, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand-x+4.5};
\addplot [red, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand-x+6.5};

%frontière
\addplot [black, very thick, domain=-3:1] {-x+1.25};
\end{axis}



\begin{axis}[xtick=\empty, ytick=\empty, xlabel={\LARGE Revenus}, ylabel={\LARGE Endettement},domain=-3:1, enlargelimits=false,ymin=-3,ymax=6]
\myGlobalTransformationbis{0}{6};

% libérales
\addplot [green, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand+x};
\addplot [green, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand+x-2};
\addplot [green, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand+x-4};
\addplot [red, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand+x+2.5};
\addplot [red, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand+x+4.5};
\addplot [red, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand+x+6.5};
\addplot [red, only marks, mark=*, samples=300, mark size=0.75,domain=-3:1]{rand+x+8.5};

%frontière
\addplot [black, very thick, domain=-3:1] {x+1.25};
\end{axis}



\end{tikzpicture}
\caption{\label{fig:ydiff} Uni-modal wages and indebtedness data generating mechanism with $y = \{0,1\}$ classes displayed in \textcolor{red}{red} and \textcolor{green}{green} respectively which depends on a third feature.}
\end{figure}








\section{Literature review} \label{sec:literature}

This section aims at providing an eluded literature review of some well-known supervised clustering approaches that could be transposed to the \textit{Credit Scoring} industry.

\subsection{Supervised generative clustering methods} \label{subsec:sup_gen}

In the preceding section, examples of classical unsupervised clustering methods were given: \gls{pca} (continuous data), \gls{mca} (categorical data) and ultimately \gls{famd} (mixed data). In this section, focus is given to supervised generative methods. Indeed, a fully generative model $p(\gls{bx},y)$, if sufficiently flexible, could have easily spotted the bottlenecks of the failures of the \gls{pca} approach illustrated on Figures~\ref{fig:xdiff} and~\ref{fig:ydiff}.

\paragraph{\glsfirst{pls}}

The \gls{pls}~\cite{wold1984collinearity} algorithm seeks to combine the strengths, in its original proposal, of \gls{pca} in explaining the variance of the features $\gls{bx}$ and regression in predicting $y$ with the resulting principal components. In a classification setting, it is termed PLS-DA where DA stands for discriminant analysis.

The main idea is to construct a first component from the sum of the univariate regressions of $\gls{bbx}_j$ on $\gls{bby}$, then a second component from the sum of the univariate regressions of $\gls{bbx}_j$ subtracted by the first component on $\gls{bby}$, and so on. In a sense, a trade-off between reconstruction quality of $\gls{bbx}$ and $\gls{bby}$ with as few components as possible is achieved.

The \gls{pls} algorithm is given in Section~\ref{app1:sec_pls} of the Appendix, Algorithm~\ref{alg:pls}. It was used in~\cite{schwartz2009human} in a classification setting which results in Figure~\ref{fig:pca_vs_pls} reproduced with permission\footnote{\copyright 2009 IEEE}. It is striking how classes are better separated when using \gls{pls}. However, this does not guarantee that the resulting inferred segments' \gls{lr} will yield better predictive performance, considering that a \textit{Credit Scoring} practitioner would effectively spot two groups in Figure~\ref{fig:pca_vs_pls} (right) and separate them on the first \gls{pls} axis being above or below a threshold of approximately $0.01$. When applied to the Automobile dataset, it does not show such spectacular results (see Figure~\ref{fig:simu_pls}).

\begin{figure}
\includegraphics[width = \textwidth]{figures/chapitre6/pca_vs_pls.png}
\caption{Cloud points resulting from the application of \gls{pca} (left) and \gls{pls} (right) on a binary-labelled multivariate continuous dataset.}
\label{fig:pca_vs_pls}
\end{figure}

\begin{figure}
\centering \resizebox{.8\textwidth}{!}{\input{R_CODE_FIGURES/chapitre6/graph_pls.tex}}
\caption{Cloud points resulting from the \gls{pls} algorithm applied to the running example of the Automobile dataset with good and bad borrowers in \textcolor{red}{red} and black respectively.}
\label{fig:simu_pls}
\end{figure}


\paragraph{\glsfirst{spc}}

The \gls{spc}~\cite{bair2006prediction} algorithm is motivated by genomics applications where $d > n$, but is applicable to our current setting as well, and by the fact that, in a predictive setting, variance of the features $\gls{bx}$ is only interesting if correlated with $y$. The inner-workings of the algorithm are relatively simple: the correlation between each feature $\gls{x}_j$ and $y$ is computed. Only the features for which this correlation exceeds a user-defined threshold are retained, and the first few principal components of these features are calculated and used to predict $y$.

There is a close link between \gls{pls} and \gls{spc} that is thoroughly explained in~\cite{friedman2001elements} Section 18.6.2 p.\ 680. For numerical experiments of Section~\ref{sec:num_exp}, we will use, among others, the \gls{pls} approach.

\medskip

Although these methods make good use of $y$ in constructing sub-populations on which the practitioner would construct separate scorecards, the resulting segments would be, as described in the \gls{mca} Section, visually separated clouds of points on the graph of the two first principal components. This paradigm has two major drawbacks: first, as explained in the preceding section, the separation boundary is complex and multivariate (as the two first principal components will most likely involve all features). Second, to make a complete tree as on Figure~\ref{fig:arbre}, these procedures would have to be repeated ``recursively'' which yields the need for a stopping criterion and an objective splitting criterion in place of the rather subjective visual separation. Direct approaches of estimating such trees are reviewed in the next section.

\subsection{Direct approaches: logistic regression trees} \label{subsec:direct}

\paragraph{\glsfirst{lotus}}

The first research work focusing on a similar problem than the present one seems to be \gls{lotus}~\cite{chan2004lotus}, where logistic regression trees are constructed so as to select features to split the data on the tree's nodes which break the linearity assumption of \gls{lr}. The original article states an application case similar to this one, namely the insurance market.

Their motivation is that \gls{lr} has a fixed parameter space, defined by the number of input features, whereas trees adapt their flexibility (\textit{i.e.}\ depth) to the sample size $n$; however, trees perform well for classification (\textit{i.e.}\ their label estimates $\hat{y}$ can achieve low classification error) but poorly in assessing the probability of the event (\textit{i.e.}\ the estimate $\hat{p}(y | \gls{bx})$ is the proportion of the event $y$ among observations $\gls{bx}$ at each leaf which is arguably not very informative) as it is piecewise constant; if the true decision boundary separating the two classes of $y$ given $\gls{bx}$ is linear, they need an infinite depth to estimate it as well as \gls{lr}. Thus, they search for trees which leaves are \gls{lr} with a few continuous features and which intermediate nodes split the population based on categorical or continuous features which relationship to the log-odd ratio of $y$ is not linear (\textit{i.e.}\ features that would perform poorly in a \gls{lr}).

They propose a feature selection method for node splits that is claimed to be ``bias-free'': as seen in Chapters~\ref{chap4} and~\ref{chap5}, the number of partitions of $\gls{lj}$ labelled factor levels into $m_j$ unlabelled categories (which would here be the tree split criterion and define its sub-populations) is huge which yields overfitting; thus, their approach relies on a $\chi^2$ test which degrees of freedom is linked to the number of potential rearrangements of $\gls{lj}$ levels into 2 bins to avoid wrongfully selecting categorical features that have lots of levels. Their optimized criterion is the sum of the log-likelihoods of the \gls{lr} on the tree's leaves. Of course, this leads also to overfitting which requires the tree to be pruned (as is classical for classification trees) using a method closely related to the one developed in the classical CART~\cite{cart84} algorithm. Lastly, their proposed method is not directly applicable to missing values: these observations are not used during training (in the \textit{Credit Scoring} industry, there would most likely be at least one missing value for each observation) and during test, their missing values are imputed by the mean or median.

To sum up, although their motivation is similar to the present problem, \gls{lotus} is not directly usable since only continuous features are used as predictive features in the \gls{lr} of the tree's leaves, it does not handle missing values gracefully, and there are currently no implementation available in \textsf{R} or Python.

\paragraph{\glsfirst{lmt}}

The second approach very close to our industrial problem is named \gls{lmt}~\cite{landwehr2005logistic}. As for~\gls{lotus}, the result is a tree of \gls{lr} at its leaves and the motivation is very similar. Their introductory example, reproduced here with permission on Figure~\ref{fig:lmt} is enlightening: a quadratic bivariate boundary cannot be well approximated by trees or \gls{lr} alone, but a combination of both achieves good performance and interpretation.

Their approach differs however drastically from \gls{lotus} in that they rely on a particular boosting approach derived from the LogitBoost algorithm~\cite{friedman2000additive} to adjust the \gls{lr}, and an adaptation of the classical C4.5~\cite{quinlan2014c4} algorithm to grow the tree. The two central ideas behind their usage of the LogitBoost algorithm, reproduced in Algorithm~\ref{alg:logitboost} of Section~\ref{LogitBoost} of the appendices, are simple: it allows to perform feature selection \textit{via} a stagewise-like process where one feature enters the model at each step and to recursively ``refine'' the \gls{lr} by boosting the \gls{lr} fitted at a node's parent. Indeed, a first \gls{lr} is fitted at the tree's root via LogitBoost using all observations $(\gls{bbx},\gls{bby})$, which is further boosted separatly at its subsequent children nodes on sub-populations, say $((\gls{bbx}^1,\gls{bby}^1), (\gls{bbx}^2,\gls{bby}^2))$ and so on. This most probably induces less parameter estimation variance in each leaf since they partly benefit from samples not in their leaf but used to fit the parents' \gls{lr}. One if its main advantages compared to other approaches is that it is fast. Here again, the resulting tree must be pruned and either a tactic similar to the classical tree algorithm CART, or cross-validation, or the AIC criterion (in a refinement of the method proposed in~\cite{sumner2005speeding}) are used.

However, categorical features are dummy / one-hot encoded so that only a few factor levels might be selected at each leaf, which amounts to merging the not selected levels into a reference value. Conversely, when used as a split feature, each level yields a distinct branch. Moreover, missing values are imputed by the mean or mode.

Its original implementation is in Java (Weka) but the \textsf{R} package \rinline{RWeka} provides interfaces and wrappers to it. When applied directly to the Automobile dataset, as \gls{lmt} does not handle missing values, a first preprocessing step is to select only complete observations: there remains only approx.\ $4{,}000$ observations (among $50{,}000$) and no segmentation is performed. Due to the use of the LogitBoost algorithm, only a few features are selected: one continuous feature and three particular levels of three different categorical features, yielding a rather low performance of $44.3$ Gini points (compared to the current performance of $55.7$) which is nevertheless impressive given the few training observations and features used. To help the \gls{lmt} algorithm further, features with the highest rate of missing values are deleted; we now have $d = 21$ and $n = 20{,}000$. Finally, in a third experiment, missing values of categorical features are encoded as a particular level and continuous features' missing values are imputed by their mean, all observations and features can now be used. In these two last experiments, the LogitBoost algorithm seems to fail since no segmentation is performed and only the age of the client is retained, yielding a low performance ($30$ Gini points).

\begin{figure}[!htb]
{\setlength{\parindent}{0cm}}
\begin{center}
%\begin{multicols}{3}
\centering
\begin{subfigure}[t]{0.25\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/chapitre6/lmt_generation.png}
\caption{\label{fig:lmt1} Quadratic data generation process: the true boundary depends on the square of input features.}
\end{subfigure}%
%\columnbreak
\hspace*{1cm}
\begin{subfigure}[t]{0.25\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/chapitre6/lmt_tree_1.png}
\caption{\label{fig:lmt2} The first split of a classification tree is way too simplistic.}
\end{subfigure}
%\columnbreak
\hspace*{1cm}
\begin{subfigure}[t]{0.25\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/chapitre6/lmt_tree_2.png}
\caption{\label{fig:lmt3}  The second split is more helpful.}
\end{subfigure}
%\end{multicols}
\end{center}

\begin{center}
%\begin{multicols}{2}
\centering
\begin{subfigure}[t]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/chapitre6/lmt_tree_3.png}
\caption{\label{fig:lmt4} The subsequent splits yield overfitting: these nodes shall be pruned.}
\end{subfigure}%
%\columnbreak
\hspace*{1cm} \begin{subfigure}[t]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/chapitre6/lmt_logistic.png}
\caption{\label{fig:lmt5} A \gls{lr} tree with only two leaves (and consequently two \gls{lr}) yields the best result.}
\end{subfigure}
%\end{multicols}
\end{center}

\caption{\label{fig:lmt} LMT motivational example.}
\end{figure}




\paragraph{\glsfirst{mob}} \label{par:mob}

Lastly, a third approach closely related to our problem is \gls{mob}~\cite{zeileis2008model} which is an adaptation of a better-known paper~\cite{hothorn2006unbiased} to parametric models in the leaves of a recursively partitioned dataset (hence the name).

Their algorithm consists in fitting the chosen model (in our case, \gls{lr}) for all observations $(\gls{bbx},\gls{bby})$ at the current node and decide to split these into subsets based on a correlation measure (several such measures are proposed) of the residuals of the current model and splitting features $\bm{V} = (V_1, \dots, V_p)$ where $V_j \in \gls{R}$ or $\gls{NO}$, $1 \leq j \leq p$, which are not necessarily included in $\gls{bbx}$ (they are specified by the user). The procedure is repeated until no significant ``correlation'' is detected. The C4.5 algorithm, in presence of a binary outcome, orders the levels of categorical features by their proportion of events $y$ and split as if the feature were ordinal (it can be shown that it is optimal, see~\cite{friedman2001elements} Section 9.2.4).  Similarly to \gls{lotus} and contrary to C4.5, \gls{mob} performs, for example for binary splits and when confronted to categorical features $j$ having $\gls{lj}$ levels, $2^{\gls{lj}}$ tests. Moreover, there is no mention of an eventual treatment of missing values. Finally, the number of segments per split is searched exhaustively.

Its implementation is available through the \textsf{R} packages \rinline{party} and \rinline{partykit} which will be used in numerical experiments of Section~\ref{sec:num_exp}. It worked well on small toy data, however, on real data, even with complete cases ($n = 4{,}000$) and by arbitrarily selecting $d = 4$ features, computation took a very long time. With bigger datasets, I got ``file size''-related errors.

\medskip

To sum up, these direct approaches are far more promising than unsupervised and supervised generative approaches of Sections~\ref{subsec:adhoc} and~\ref{subsec:sup_gen} respectively, in that they produce directly the sought tree-structure of Figure~\ref{fig:arbre} (apart, of course, from quantization $\q^1, \dots, \q^7$ and interactions $\bdelta^1, \dots, \bdelta^7$). However, their treatment of missing values and categorical features are not satisfactory: classical \textit{Credit Scoring} data would require preprocessing steps such as imputation or quantization (or at least merging numerous factor levels) which might greatly influence the resulting segmentation as emphasized in Section~\ref{subsec:context}. Moreover, quantization has to be segment-specific: on a theoretical note, it participates in reducing model bias; on a practical note, it does not make much sense to use the same quantization of wages on segments of applicants to a leasing for a Ferrari or for a smartphone.

In the next section, we formalize the problem as a model selection problem, similarly to the three approaches presented here, with our own notations introduced in the previous section and chapters.


\section{Logistic regression trees as a combinatorial model selection problem} \label{sec:model_selec_tree}

We assume there are $\gls{K}^\star$ ``clusters'' which form the leaves of a tree similar to Figure~\ref{fig:arbre} and which assigning latent random feature is denoted by $\gls{C}^\star$ (lower-case for observations). The other notations employed inspire from the preceding chapters: the superscript notation is used to insist on the fact that available features $\gls{bx}^{c^\star}$ differ potentially in each of the scorecards. For $c^\star \in \gls{N}_{\gls{K}^\star}$, $(\gls{bbx}^{c^\star}, \gls{bby}^{c^\star})$ denotes the subset of observations of $(\gls{bbx}, \gls{bby})$ for which $\gls{C}^\star = c^\star$, such that $ \bigsqcup_{c^\star=1}^{\gls{K}^\star} (\gls{bbx}^{c^\star}, \gls{bby}^{c^\star}) = (\gls{bbx}, \gls{bby})$. It follows that quantizations $\q^{c^\star}$ and interactions $\bdelta^{c^\star}$, discussed in Chapters~\ref{chap4} and~\ref{chap5} respectively are also different. Consequently, the \gls{lr} coefficients $\gls{bth}^{\star,c^\star}$ are also obviously different. In this section, we drop the quantization and interactions requirement, such that:
\begin{equation} \label{eq:lr_tree}
\forall \gls{bx}, y, \exists c^\star \in \gls{N}_{\gls{K}^\star}, \gls{bth}^{\star,c^\star}, p(y | \gls{bx}) = p_{\gls{bth}^{\star,c^\star}}(y | \gls{bx})
\end{equation}
%In this Section, it is supposed the well-specified model is indeed a \gls{lr} tree, which amounts to assuming there $K^\star$ segments. 
The membership of an observation $\gls{bx}$ to a segment $\gls{c}$ is given by a tree. We restrict to binary trees for simplicity, such that a segment $\gls{c}$ with depth $\mathcal{D}(c)$ has $r = 1, \dots, \mathcal{D}(c)$ parents successively denoted by $\mathcal{P}a^r(c)$. At these parent nodes, a binary rule is taken. This rule is univariate: it depends on only one feature $\gls{x}_{\sigma(r,c)}$ where $\sigma(r,c)$ denotes the anti-rank of the feature used in rule $r$ for segment $\gls{c}$. Being a binary rule, the membership of $\gls{x}_{\sigma(r,c)}$ is tested between $C_{\mathcal{P}a^r(c),1}$ and $C_{\mathcal{P}a^r(c),2}$ such that $C_{\mathcal{P}a^r(c),1} \bigsqcup C_{\mathcal{P}a^r(c),2} =\gls{R}$ for continuous features (half-spaces), or $\gls{N}_{l_{\sigma(r,c)}}$ for categorical features respectively. 
%The `unique `side'' $1$ or $2$ of rule $r$ that yields eventually to $\gls{c}$ is denoted by $\lambda(r,c)$. 
This membership is denoted by $\lambda(r,c)$ such that $\gls{x}_{\sigma(r,c)} \in C_{\mathcal{P}a^r(c),\lambda(r,c)}$.
With all these newly introduced notations, the probability of a segment $\gls{c}$ given covariates $\gls{bx}$ can be expressed as:
\begin{equation} \label{eq:tree}
p( c | \gls{bx}) = \prod_{r = 1}^{\mathcal{D}(c)} \mathds{1}_{C_{\mathcal{P}a^r(c),\lambda(r,c)}} (\gls{x}_{\sigma(r,c)})
\end{equation}
%where $r$ denotes the inverse depth of each rule relative to $\gls{c}$ ($r=\ln_2 K$ gives the root rule, and so on) and $\sigma(r,c)$ gives the anti-rank of the feature used in rule $r$, at node $\mathcal{P}a^r(c)$, $C_{\mathcal{P}a^r(c),\lambda(r,c)} = \begin{cases} ]-\infty ; \lambda_r] \\ \text{or} \\ ] \lambda_r ; +\infty[ \end{cases}$ for a continuous feature and $C_{r,c} = \begin{cases} \lambda_{r,1} \\ \text{or} \\ \lambda_{r,2} \end{cases}$ such that $ \lambda_{r,1} \bigsqcup \lambda_{r,2} = \gls{N}_{l_{\sigma(r)}}$ for a categorical feature depending on the ``side'' of the segment $\gls{c}$ w.r.t.\ the rule $r$.
An example is given on Figure~\ref{fig_arbre_ex_notations}.
%In each segment, the well-specified assumption is equivalent to a \gls{lr}:
%\begin{equation} \label{eq:lr_tree}
%\forall \gls{bx}, y, \exists c^\star \in \gls{N}_{K^\star}, \gls{bth}^{\star,c^\star}, p(y | \gls{bx}) = p_{\gls{bth}^{\star,c^\star}}(y | \gls{bx}, c^\star)
%\end{equation}
%For $c \in \gls{N}_{K^\star}$, $(\gls{bbx}^c, \gls{bby}^c)$ denotes the subset of observations of $(\gls{bbx}, \gls{bby})$ for which $c^\star = c$, such that $ \bigsqcup_{c=1}^{K} (\gls{bbx}^c, \gls{bby}^c) = (\gls{bbx}, \gls{bby})$.

\tikzstyle{level 1}=[level distance=2.2cm, sibling distance=8cm]
\tikzstyle{level 2}=[level distance=2.5cm, sibling distance=4cm]


\begin{figure}
\resizebox{\textwidth}{!}{
\centering
\begin{tikzpicture}
  [
    sibling distance        = 15em,
    level distance          = 5em,
    edge from parent/.style = {draw, -latex},
    every node/.style       = {font=\footnotesize},
    sloped
  ]
  \node [root] {\textcolor{black}{\small $\begin{array}{c} r = 2 \\ \lambda(2,1) = 1 \end{array}$}}
    child { node [dummy] {\small $\begin{array}{c} r = 1 \\ \lambda(1,1) = 1 \end{array}$}
      child { node [env] {\textcolor{black}{$c=1$ \hspace*{-0.4cm}}}
          edge from parent node [below] {$\gls{x}_{\sigma(1,1)} \in C_{1,1}$} }
      child { node [env] {\textcolor{black}{$c=2$ \hspace*{-0.4cm}}}
              edge from parent node [above, align=center] {$\gls{x}_{\sigma(1,1)} \not\in C_{1,2}$} }
              edge from parent node [above] {$\gls{x}_{\sigma(2,1)} \in C_{2,1}$} }
    child { node [dummy] {$\cdot$}
%      child { node [env] {\textcolor{black}{$c=3$ \hspace*{-0.4cm}}}
%          edge from parent node [above] {$\cdot$} }
%      child { node [env] {\textcolor{black}{$c=4$ \hspace*{-0.4cm}}}
%              edge from parent node [above, align=center] {$\cdot$} }
              edge from parent node [above] {$\gls{x}_{\sigma(2,1)} \not\in C_{2,2}$} };
\end{tikzpicture}
}
\caption{Notations of a segmentation tree by an example.}
\label{fig_arbre_ex_notations}
\end{figure}

The above mentioned algorithms \gls{lotus}, \gls{lmt} and \gls{mob} optimized the sum of the segments' log-likelihoods, then needed pruning since it leads to obvious overfitting: infinite log-likelihood is achievable by putting each sample into its own segment, provided there is at least one continuous feature and no identical examples with different labels, or combinations of categorical features' levels that separate classes perfectly.

Another approach can be taken by considering the segment $\gls{c}$ as a latent random feature:
\begin{align}
p(\gls{bbx},\gls{bby}) & =  \sum_{c=1}^{\gls{K}^\star} p(\gls{bby} | \gls{bbx}, c) p(c | \gls{bbx}) p(\gls{bbx}) & \text{($p(c | \gls{bx})$ is non-zero only for $c = c^\star$)} \nonumber \\
 & = \prod_{c^\star=1}^{\gls{K}^\star} p(\gls{bby}^{c^\star} | \gls{bbx}^{c^\star}, c^\star) p(\gls{bbx}) \nonumber \\
 & = \prod_{c^\star=1}^{\gls{K}^\star} \int_{\Theta_{c^\star}} p_{\gls{bth}^{c^\star}}(\gls{bby}^{c^\star} | \gls{bbx}^{c^\star}) p(\gls{bth}^{c^\star} | c^\star) d\gls{bth}^{c^\star} p(\gls{bbx}) \label{eq:likelihood_segment}
%\ln p(\gls{bby}, \gls{bbx}) & = \sum_{c=1}^{\gls{K}^\star} \exp \left( \frac{-\text{BIC}(\hat{\gls{bth}^c}) + O(1)}{2} \right) + \ln p(\gls{bbx}) 
\end{align}
Consequently, the following criterion can be used to select a segmentation:
\begin{equation} \label{eq:BICc}
(\gls{K}^\star, c^\star) = \argmin_{\gls{K},c} \sum_{c=1}^{\gls{K}} \text{BIC}(\hat{\gls{bth}}^c) + \gls{K} - 1
\end{equation}
As was thoroughly explained for quantizations and interactions in Sections~\ref{par:consistency} and~\ref{sec:pairwise} respectively, it is unclear how many parameters should be accounted for in this BIC criterion since the tree of Equation~\eqref{eq:tree} has ``parameters'', in the sense that it selects a splitting feature and a splitting criterion, 
%$C_{\cdot,\cdot}$
which have to be estimated (this is somewhat reflected in Equation~\eqref{eq:likelihood_segment} by the $p(\gls{bth}^c | c)$ term); some are continuous (when the split is done on a continuous feature), some are discrete (when it concerns a categorical feature). As discussed in Section~\ref{par:consistency}, discrete parameters are usually not counted, but here, following the C4.5 approach of considering the levels of categorical features as ordered (w.r.t.\ the proportion of events $y$ associated to them - see Section~\ref{subsec:sup_gen}, Paragraph~\nameref{par:mob}), a split on categorical features can count as one continuous parameter. This yields the $\gls{K}-1$ term in Criterion~\eqref{eq:BICc}. However, when there are more than two classes $\gls{c}$ (typically, a financial institution of moderate to big size would have $\gls{K} = 4$ to $30$ scorecards), this ``ordering'' simplification about the search for discrete parameters does not apply. We still stick with criterion~\eqref{eq:BICc} as it will show good empirical properties in Section~\ref{sec:num_exp}.


In the next section, we propose to relax the constraint of Equation~\eqref{eq:tree}, exactly as was done for quantizations in Chapter~\ref{chap4}, by using a continuous approximation of this discrete problem (and thus highly difficult to optimize directly).


\section{A mixture and latent feature-based relaxation}

The difficulty in optimizing Criterion~\eqref{eq:BICc} directly lies in the discrete nature of $\gls{c}$ given $\gls{bx}$, illustrated by the profusion of indicator functions in Equation~\eqref{eq:tree}, which is very similar to the problems of quantization (see Chapter~\ref{chap4} and in particular Section~\ref{subsec:relaxation}) and interaction screening (see Chapter~\ref{chap5} and in particular Section~\ref{subsec:mcmc}). In both cases, highly-combinatorial discrete problems were relaxed, by approaching door functions by softmax and relying on MCMC methods. Unsurprisingly, a similar approach can be taken here to design a smooth approximation of $p(c | \gls{bx})$ which will be denoted by $p_{\betag}(c | \gls{bx})$.

\subsection{The proposed relaxation: tree structure and piecewise constant membership probability} \label{subsec:relax_tree}

As emphasized in Section~\ref{subsec:sup_gen}, classification trees aim at predicting $\gls{c}$ by making their leaves as ``pure'' as possible (hence the use of the term ``impurity measure'' to designate their optimized criterion), \textit{i.e.}\ where one class strongly dominates the others by being the labels of most observations that fall into it. However, as for \gls{lr}, they can be viewed as probabilistic classifiers by substituting their classical majority vote by the proportion of each class in each leaf:
\begin{equation}
p_{\betag}(c | \gls{bx}) = \frac{|\mathbf{c}^{\mathcal{L}(\gls{bx})}|}{|\gls{bbx}^{\mathcal{L}(\gls{bx})}|},
\end{equation}
where $\mathcal{L}(\gls{bx})$ denotes the leaf where $\gls{bx}$ falls and $\beta$ is sloppily used to denote all parameters involved in classical classification tree methods such as CART and C4.5 written explicitly in Equation~\eqref{eq:tree}. Indeed, in this soft assignment, $\mathcal{L}(\gls{bx})$ and its segment $\gls{c}$ are not identifiable anymore. An example of such behaviour is given on Figure~\ref{fig:titanic_tree} where there are two classes: ``survived'' and ``not survived'' for Titanic passengers given their age, sex and passenger class. The proportion of each class in each leaf is given in parentheses.

\begin{figure}
\centering \resizebox{\textwidth}{!}{\input{R_CODE_FIGURES/chapitre6/titanic_tree.tex}}
\caption{A C4.5 decision tree applied to the famous Titanic dataset containing the fate of 1309 passengers alongside their class, age and sex.}
\label{fig:titanic_tree}
\end{figure}

This ``soft'' assignment will be useful to design an algorithm that does not greedily evaluate all possible segmentations of the form of Equation~\eqref{eq:tree} and its subsequent \gls{lr}. A softmax could have been used similarly as in Chapter~\ref{chap4} but would have yielded a major drawback: the assignment decisions would have been multivariate, thus losing the interpretability of the tree structure. Using this new parametrization, we get a mixture model:
\begin{align*}
p(y | \gls{bx}) & = \sum_{c = 1}^{\gls{K}} p_{\gls{bth}^c}(y | \gls{bx}) p_{\betag}(c | \gls{bx}),
\end{align*}
where feature $\gls{c}$ is latent and which makes immediately think of a straightforward estimation strategy: the \gls{em} algorithm. Indeed, it can be easily remarked that:
\begin{align*}
p(c | \gls{bx}, y) & \propto p_{\gls{bth}^c}(y | \gls{bx}) p_{\betag}(c | \gls{bx}),
\end{align*}
which will be at the basis of the \gls{em}'s fuzzy assignment among segments, detailed in the next section.

\subsection{A classical \gls{em} estimation strategy}

The \gls{em} algorithm~\cite{dempster1977maximum} is an iterative method that can be used to estimate the \textit{maximum a posteriori} of $p(c | \gls{bx}, y)$, since $\gls{c}$ is latent, and alternates between the expectation (E-)step, which computes the relative membership of the observations into each segment, and a maximization (M-)step, which computes the \gls{mle} of the parameters of the log-likelihoods of each segment's \gls{lr} and the tree structure. These new \gls{lr} and tree estimates are then used to determine the distribution of the latent variables in the next E-step. Considering the number of segments $\gls{K}$ fixed, the E- and M-steps of the \gls{em} can be derived as follows.

\paragraph{E-step}
At iteration $(s+1)$, the membership of an observation $i$ to segment $\gls{c}$ can be computed as:
\[ t_{i,c}^{(s+1)} = \frac{p_{\gls{bth}^{c(s)}}(y_i | \gls{bx}_i) p_{\betag^{(s)}}(c | \gls{bx}_i) }{ \sum_{c'=1}^{\gls{K}} p_{\gls{bth}^{c{'}{(s)}}}(y_i | \gls{bx}_i) p_{\betag^{(s)}}(c{'} | \gls{bx}_i) }.\]

\paragraph{M1-step}
The previous E-step allows to derive the new \gls{mle} of the \gls{lr} parameters of each segment $\gls{c}$ as:
\[ \gls{bth}^{c(s+1)} = \argmax_{\gls{bth}} \sum_{i=1}^n t_{i,c}^{(s+1)} \ln p_{\gls{bth}^c}(y_i | \gls{bx}_i). \]

\paragraph{M2-step}
Similarly, a new tree structure can be derived by the new \gls{mle} of its parameter $\betag$:
\[ \betag^{(s)} = \argmax_{\betag} \sum_{i=1}^n \sum_{c=1}^{\gls{K}} t_{i,c} \ln p_{\betag}( c | \gls{bx}_i). \]
Unfortunately, tree induction methods like CART or C4.5 do not follow a maximum likelihood approach, so that they rather try to minimize a so-called impurity measure, the Gini index or the entropy, respectively. However, since it is hoped that segments $c^\star$ are ``peaks'' of the distribution $p_{\betag}( c | \gls{bx})$, just as it was supposed that the best quantization $\bqk^\star$ dominated its posterior \gls{pdf} in the \gls{sem} algorithm proposed in Section~\ref{subsec:fuzzy}, the log-likelihood can be well approximated by the entropy, such that:
\[ \betag^{(s+1)} \approx \argmax_{\betag} \sum_{i=1}^n \sum_{c=1}^{\gls{K}} t_{i,c}^{(s+1)} \underbrace{p_{\betag}( c | \gls{bx}_i)}_{\begin{cases} \approx 1 \text{ for } c = c^\star, \\ 0 \text{ otherwise.} \end{cases}} \ln p_{\betag}( c | \gls{bx}_i). \]
This last formulation allows to obtain $\betag^{(s)}$ from a simple application of the C4.5 algorithm, with observations properly weighted by $t_{i,c}$.

However, this approach suffers from two main drawbacks: first, all observations are used in all \gls{lr} $p_{\gls{bth}^c}$ which might be problematic with real data since there will be ``blocks'' of available features (\textit{e.g.}\ vehicle information); second, all possible values of $\gls{K}$ must be iterated through since the \gls{em} algorithm does not allow for the disappearance of a segment $\gls{c}$ contrary to the \gls{sem} approach developed hereafter.

\subsection{An \gls{sem} estimation strategy} \label{subsec:sem}

In a similar fashion as the MCMC approaches developed in Chapters~\ref{chap4} and~\ref{chap5} where a ``clever'' quantization (resp.\ interaction matrix) was drawn and evaluated at each step, refining it for the subsequent steps, a straightforward way of building logistic regression trees is to propose a tree structure, fit \gls{lr} at its leaves, and evaluate the goodness-of-fit using Criterion~\ref{eq:BICc} of the resulting logistic regression tree. This is somehow the way \gls{lmt} works: a tree structure is proposed based on C4.5, \gls{lr} are fitted using the LogitBoost algorithm, and the tree is pruned back using a goodness-of-fit criterion.

Similarly to the quantization and the interaction screening problems, doing so for all possible tree structures is intractable, so that a way of generating ``good'' candidates can be designed by relying on an \gls{sem} algorithm, which we call \textit{glmtree}. The E-step of the previous Section is thus replaced by a Stochastic (S-) step which has some consequences on the M-steps.
\paragraph{S-step} The ``soft'' assignment of the \gls{em} algorithm of the previous Section is hereby replaced by a ``hard'' stochastic assignment such that:
\[ c_i^{(s+1)} \sim p_{\gls{bth}^{\cdot(s)}}(y_i | \gls{bx}_i) p_{\betag^{(s)}}(\cdot | \gls{bx}_i). \]
\paragraph{M1-step} Thanks to the previous step, the segments are now ``hardly'' assigned such that the \gls{lr} are estimated using only observations affected to their segment:
\[ \gls{bth}^{c(s+1)} = \argmax_{\gls{bth}} \ell(\gls{bth};\gls{bbx}^{c(s+1)},\gls{bby}^{c(s+1)}). \]
\paragraph{M2-step} Similarly, a new tree structure is given by:
\[ \betag^{(s)} = \argmax_{\betag} \ell(\betag, \gls{bbx}, \mathbf{\bm{c}}). \]
This last expression is again approximated by C4.5's impurity measure: the entropy. Without more theoretical and empirical work, it is unclear which of the \gls{em} and \gls{sem} approaches will perform best. However, as mentioned earlier, this \gls{sem} algorithm calls for an easy integration with the quantization and interaction screening methods proposed in the previous chapter.

\subsection{Choosing an appropriate number of ``hard'' segments} \label{subsec:hard_seg}

\paragraph{Going back to ``hard'' segments}
The motivation of Section~\ref{subsec:relax_tree} was to propose a relaxation of Equation~\eqref{eq:tree} so that an iterative estimation, be it an \gls{em} or an \gls{sem} algorithm, could be carried out. In Chapter~\ref{chap4}, a similar relaxation was proposed for quantization, which lead us to propose a ``soft'' quantization $\q_{\ag}(\cdot)$ or $p_{\ag}(\bqk_j | \cdot)$ for the neural network and the \gls{sem} approaches respectively. These relaxations allowed quantized features to be ``partly'' in all intervals or groups for continuous or categorical features respectively. Thus, to get back to the original quantization problem, a \textit{maximum a posteriori} scheme was introduced in Section~\ref{subsec:relaxation} to deduce ``hard'' quantizations from this relaxation. In our tree setting, a similar approach has to be taken: this soft segmentation can be interpreted as a mixture of \gls{lr} which implies that all applicants are scored by all scorecards which is arguably not interpretable. An assignment of each applicant $i$ to a single scorecard, \textit{i.e.}\ to a leaf of the segmentation tree, is easily done again by a \textit{maximum a posteriori} step such that:
\begin{equation} \label{eq:max_seg}
\hat{c}_i^{(s)} = \argmax_c p_{\betag^{(s)}}(c | \gls{bx}_i).
\end{equation}

\paragraph{Segmentation candidates}
Similarly to the neural network architecture introduced in Section~\ref{sec:proposal}, the \gls{sem} algorithm which proposed quantization candidates introduced in Section~\ref{sec:sem} and the Metropolis-Hastings algorithm for pairwise interaction screening introduced in Section~\ref{subsec:mcmc}, the \gls{em} and \gls{sem} strategies introduced in the two previous sections for segmentation are merely ``segments providers''. Indeed, through the iterations $1$ to $S$, as argued in the preceding paragraph, segmentations $\hat{\mathbf{c}}^{(1)}, \dots \hat{\mathbf{c}}^{(S)}$ are proposed through a \textit{maximum a posteriori} rule parallel to these algorithms. These candidates are then reintroduced to our original criterion~\eqref{eq:BICc} and the best performing segmentation is found according to:
\begin{equation} \label{eq:BICtree}
s^\star = \argmin_s \text{BIC}(\hat{\gls{bth}}^{c^{(s)}}) + \gls{K}^{(s)} - 1,
\end{equation}
which bears resemblance with Equation~\eqref{eq:opt_epoch} for quantizations.

\paragraph{Exploring a fewer number of segments}
In the preceding sections, the number of segments $\gls{K}$ was assumed to be fixed. However, the \textit{maximum a posteriori} scheme introduced in this section allows, similarly to the one used to go from ``soft'' ($\q_{\ag}(\cdot)$ or $p_{\ag}(\bqk_j | \cdot)$) to ``hard'' ($\q(\cdot)$) quantizations, to explore a number of segments potentially way lower than $\gls{K}$: for a fixed segment $\gls{c}$, if there is no observation $i$ such that $p_{\betag}(c | \gls{bx}_i)$ > $p_{\betag}(c' | \gls{bx}_i)$ for $c' \neq c$, than the segment is empty, which is equivalent to producing a segmentation in $\gls{K}-1$ segments. Supplemental to this thresholding effect, the use of an \gls{sem} algorithm makes it possible to enforce this phenomenon: as $\gls{c}$ is drawn in the S-step, and as was argued for quantizations with an \gls{sem} algorithm in Section~\ref{sec:sem}, Paragraph~\nameref{par:choosing_sem}, there is a non-zero probability of not drawing a particular segment $\gls{c}$ at a given step $(s)$. When run long enough, the chain will stop with $\gls{K} = 1$, just like the \textit{glmdisc}-SEM algorithm could be run until all features are quantized in one level. This can be seen as a strength since it does not require to loop on the number of segments $\gls{K}$ which would be required for an \gls{em} algorithm, which is why focus is given to the \gls{sem} algorithm in what follows.


\section{Extension to quantization and interactions} \label{sec:adding_quant}

The \gls{sem} estimation strategy proposed in the previous section has one clear advantage: it could easily be used in conjunction with the \textit{glmdisc}-SEM algorithm proposed in Chapters~\ref{chap4} and~\ref{chap5} for quantization and interaction screening.

The following modifications would have to be performed:
\paragraph{S1-step} The segment is drawn, for an observation $i$ such that $\gls{bx}_i$ belongs to segment $\gls{c}$, according to:
\[ c_i^{(s+1)} \sim p_{\gls{bth}^{\cdot(s)}}(y_i | \bqk^{\cdot(s)},\bdelta^{\cdot(s)}) p_{\betag^{(s)}}(\cdot | \gls{bx}_i). \]
\paragraph{S2-step} The \textit{glmdisc}-SEM performs the subsequent S-steps. The quantization is drawn according to:
\[ \bqk_{i,j}^{c(s+1)} \sim p_{}(y_i | \bqk_{i,-\{j\}}, \cdot, \bdelta^{c(s)}) p_{\ag_j^{c(s)}}(\cdot | \gls{xij}). \]
\paragraph{S3-step} The interaction matrix is drawn following the Metropolis-Hastings approach developed in the preceding Chapter and denoted for simplicity as MH here:
\[ \bdelta^{c(s+1)} \sim \text{MH}(\bdelta^{c(s)}, \bbqk^{c(s+1)}, \gls{bby}^{c(s+1)}). \]
\paragraph{M1-step} The \gls{lr} parameters are obtained in each segment by using the appropriate quantization, interaction matrix and observations:
\[ \gls{bth}^{c(s+1)} = \argmax_{\gls{bth}} \ell(\gls{bth};\gls{bbx}^{c(s+1)},\gls{bby}^{c(s+1)}, \bdelta^{c(s+1)}). \]
\paragraph{M2-step} In each segment and for each predictive feature in this particular segment, polytomous logistic links are fitted between the ``soft'' quantization and the raw feature:
\[ \ag_j^{c(s+1)} = \argmax_{\ag_j} \ell(\ag_j;\gls{bbx}_j^{c(s+1)},\bbqk_j^{c(s+1)}). \]
\paragraph{M3-step} The tree-structure is obtained again via the C4.5 algorithm as an approximation of:
\[ \betag^{(s+1)} = \argmax_{\betag} \ell(\betag, \gls{bbx}, \mathbf{\bm{c}}^{(s+1)}). \]
As proposed in Chapter~\ref{chap4}, parallel to this \gls{sem} algorithm, ``hard'' quantizations are obtained by performing a \textit{maximum a posteriori} operation on the quantization probability $p_{\ag}$ (see Section~\ref{subsec:relaxation}):
\[ \hat{q}_{j,h}^{c(s)}(\cdot) = 1 \text{ if } h = \argmax_{1 \leq h' \leq m_j} p_{{\ag}_{j,h'}^{(c(s)}}(\bm{e}_{h'}^{m_j} | \cdot), 0 \text{ otherwise.} \]
As proposed in Section~\ref{subsec:hard_seg}, ``hard'' segments are obtained \textit{via} a \textit{maximum a posteriori} operation on the segmentation probability $p_{\betag}$ (see Equation~\eqref{eq:max_seg}). The best logistic regression tree is thereafter chosen via the following BIC criterion~\eqref{eq:BICtree} adapted from Equations~\eqref{eq:BICc} and~\eqref{eq:BICq}.

Although this extension seems straightforward, it is relatively computationally expensive since at each step (s), $\gls{K}$ Metropolis-Hastings steps have to be performed and a tree, $\gls{K}$ \gls{lr} and $\gls{K} \times d$ polytomous logistic regressions are fitted. With a relatively small number of segments, \textit{i.e.}\ 4 to 30 as proposed earlier, it seems nevertheless feasible but it will require more work.

\section{Numerical experiments} \label{sec:num_exp}

This chapter is based on more recent work which consequently limits the exhaustiveness of the numerical experiments. The next section aims at comparing the proposed approach to other methods on simulated data from the proposed model, and in particular the failing situations discussed in Section~\ref{subsec:fail}.

\subsection{Empirical consistency on simulated data} \label{subsec:num_sim}

As for the two preceding chapters, the first set of numerical experiments are dedicated to verifying empirically the consistency of the proposed approach. To do so, we simulate the failing situations presented in Section~\ref{subsec:fail}.

\begin{enumerate}[(a)]
\item Two covariates $(\gls{x}_1,\gls{x}_2)$ are independently simulated from an equally probable mixture of $\mathcal{N}(3,1)$, $\mathcal{N}(6,1)$ and $\mathcal{N}(9,1)$ and the log odd ratio of $y$ is given by $\theta_0 + \theta_1 \gls{x}_1 + \theta_2 \gls{x}_2$ where $\theta_0 = 3$, $\theta_1 = 0.5$ and $\theta_2 = -1$. This data generating mechanism is illustrated in Figure~\ref{fig:simu_pas}. Results of various clustering methods developed in this chapter are given in Table~\ref{tab:num_exp_tree_pas}.
\item Two covariates $(\gls{x}_1,\gls{x}_2)$ are simulated from $\mathcal{U}(0,1)$ and a third categorical covariate $\gls{x}_3$ with 6 uniformly drawn levels. For levels $1$ and $2$ of feature $\gls{x}_3$, the log odd ratio of $y$ is given by $\theta_1 \gls{x}_1 + \theta_2 \gls{x}_2$ where $\theta_1 = -1$ and $\theta_2 = 0.5$. For levels $3$ and $4$, we have $\theta_1 = -0.5$ and $\theta_2 = 1.5$ and finally for levels $5$ and $6$, we set $\theta_1 = 1$ and $\theta_2 = -0.5$. This data generating mechanism is illustrated in Figure~\ref{fig:simu}. Results of various clustering methods developed in this chapter are given in Table~\ref{tab:num_exp_tree}.
\end{enumerate}

For both experiments, the \gls{sem} algorithm is initialized randomly with $\gls{K} = 5$ segments. In experiment (a), the proposed approach selects effectively no partitions. The \textit{maximum a posteriori} scheme of Equation~\eqref{eq:max_seg} is able, as argued in Section~\ref{subsec:hard_seg}, to make segments ``vanish'' and explore segmentations with less than $\gls{K}$ segments. In experiment (b), the proposed approach is able to recover the tree structure. Consequently, the proposed algorithm yields the best performance in both settings. As for \gls{famd} and \gls{pls} which resulting projections for experiment (a) are displayed on Figure~\ref{fig:simu_a_famd} and~\ref{fig:simu_a_pls} respectively, they form $3$ clusters and consequently the $3$ resulting \gls{lr} suffer from a higher estimation variance loosely reflected in their inferior performance in Table~\ref{tab:num_exp_tree_pas}. \gls{lmt} recovers the truth by producing a single \gls{lr} but not \gls{mob} (see Figure~\ref{fig:simu_a_mob}) which splits the data into $2$ segments. On experiment (b), \gls{famd} produces worse results than a single \gls{lr} and the benefit of using the target $y$ is clear from the result of \gls{pls} (see Table~\ref{tab:num_exp_tree}). \gls{mob} also recovers the true structure (see Figure~\ref{fig:simu_b_mob}) but not \gls{lmt} which first splitting node is a continuous feature not involved in the data generating mechanism of the segments as displayed on Figure~\ref{fig:simu_b_lmt}. For both experiments, it would be useful to report confidence intervals and or bar plots as was done in Chapters~\ref{chap4} and~\ref{chap5} to derive an empirical consistency of the proposed approach.

\begin{figure}
\centering \resizebox{.7\textwidth}{!}{\input{R_CODE_FIGURES/chapitre6/graph_pas_mix_reglog.tex}}
\caption{Cloud points of simulated data from (a) with respective labels in \textcolor{red}{red} and black.}
\label{fig:simu_pas}
\end{figure}


\begin{figure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\textwidth]{figures/chapitre6/graph_simu_1.jpg}
\caption{Representation of $y$ w.r.t.\ $(\gls{x}_1,\gls{x}_2)$ for $\gls{x}_3 \in \{1,2\}$.}
\label{fig:simu1}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\textwidth]{figures/chapitre6/graph_simu_2.jpg}
\caption{Representation of $y$ w.r.t.\ $(\gls{x}_1,\gls{x}_2)$ for $\gls{x}_3 \in \{3,4\}$.}
\label{fig:simu2}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\textwidth]{figures/chapitre6/graph_simu_3.jpg}
\caption{Representation of $y$ w.r.t.\ $(\gls{x}_1,\gls{x}_2)$ for $\gls{x}_3 \in \{5,6\}$.}
\label{fig:simu3}
\end{subfigure}
\caption{Cloud points of simulated data from (b) with respective labels in \textcolor{red}{red} and black.}
\label{fig:simu}
\end{figure}

\begin{figure}
\centering \resizebox{.7\textwidth}{!}{\input{R_CODE_FIGURES/chapitre6/graph_pas_mix_reglog_pca.tex}}
\caption{Cloud points of simulated data from (a) after applying the \gls{pca} algorithm.}
\label{fig:simu_a_famd}
\end{figure}

\begin{figure}
\centering \resizebox{.7\textwidth}{!}{\input{R_CODE_FIGURES/chapitre6/graph_pas_mix_reglog_pls.tex}}
\caption{Cloud points of simulated data from (a) with respective labels in \textcolor{red}{red} and black after applying the \gls{pls} algorithm.}
\label{fig:simu_a_pls}
\end{figure}

%\begin{figure}
%\centering \resizebox{.7\textwidth}{!}{\input{R_CODE_FIGURES/chapitre6/graph_lmt.tex}}
%\caption{Cloud points of simulated data from (a) with respective labels in \textcolor{red}{red} and black after applying the \gls{lmt} algorithm.}
%\label{fig:simu_a_lmt}
%\end{figure}

\begin{figure}
%\centering \resizebox{.7\textwidth}{!}{\input{R_CODE_FIGURES/chapitre6/graph_lmt_b.tex}}
\centering \includegraphics[scale=0.2]{R_CODE_FIGURES/chapitre6/graphLMT.png}
\caption{\gls{lmt} tree resulting from simulated data from (b).}
\label{fig:simu_b_lmt}
\end{figure}

\begin{figure}
\centering \resizebox{.7\textwidth}{!}{\input{R_CODE_FIGURES/chapitre6/graph_mob.tex}}
\caption{\gls{mob} tree resulting from simulated data from (a).}
\label{fig:simu_a_mob}
\end{figure}

\begin{figure}
\centering \resizebox{.7\textwidth}{!}{\input{R_CODE_FIGURES/chapitre6/graph_mob_b.tex}}
\caption{\gls{mob} tree resulting from simulated data from (b).}
\label{fig:simu_b_mob}
\end{figure}

\begin{table}[t]
\caption{\label{tab:num_exp_tree_pas} Comparison of several clustering approaches w.r.t.\ the subsequent predictive performance in experiment (a).}
\centering
\begin{tabular}{ll|lllll}
 & Oracle = ALLR & \textit{glmtree}-SEM & \gls{famd} & \gls{pls} & \gls{lmt} & \gls{mob} \\
\hline
Gini & 69.7 & \textbf{69.7} & 65.3 & 47.0 & \textbf{69.7} & 64.8 \\
\end{tabular}
\end{table}


\begin{table}[t]
\caption{\label{tab:num_exp_tree} Comparison of several clustering approaches w.r.t.\ the subsequent predictive performance in experiment (b).}
\centering
\begin{tabular}{ll|llllll}
 & Oracle & ALLR & \textit{glmtree}-SEM & \gls{famd} & \gls{pls} & \gls{lmt} & \gls{mob} \\
\hline
Gini & 69.7 & 25.8 & \textbf{69.7} & 17.7 & 48.4 & 65.8 & \textbf{69.7} \\
\end{tabular}
\end{table}



\subsection{Benchmark on \textit{Credit Scoring} data}

\subsubsection{The running example: the Automobile dataset}

Recall from Sections~\ref{subsec:adhoc} and~\ref{subsec:sup_gen} that \gls{pca}, \gls{mca}, \gls{famd} and \gls{pls} revealed no segments on this dataset and from Section~\ref{subsec:direct} that \gls{lmt} produced disappointing results and \gls{mob} could not be tested.

By applying \textit{glmtree}-SEM to the Automobile dataset, we get $\hat{\gls{K}} = $ segments defined by the tree given in Figure~\ref{fig:tree_sem_auto} yielding an overall performance of $ $ Gini points.

\begin{figure}
\centering

\caption{\label{fig:tree_sem_auto} bla.}
\end{figure}

\subsubsection{One year of financed applications}

A subset of all applications, representative of approx.\ $30$ portfolios with different scorecards, has been extracted for the purpose of the present benchmark with $n = 900{,}000$ observations and $d = 18$ among which $12$ continuous features and $8$ categorical features with $6$ to $100$ levels (most features are similar to the Automobile dataset). The missing values have been preprocessed such that no continuous features have missing values and the categorical features have a separate and meaningful ``missing'' level. Also for simplification purposes, no quantization or interaction screening is performed so that the \gls{sem} algorithm is conducted as presented in Section~\ref{subsec:sem}.

Generative approaches (\gls{famd} and \gls{pls}) are not used due to their subjectivity (visual separation) and the fact that they are used by practitioners to provide ``local'' segments (\textit{e.g.}\ for the Automobile market). Hence for such a large dataset, they would have to be applied ``recursively'' (applying \gls{famd} / \gls{pls} on each of the resulting visually separated segments). For computational reasons that became apparent in applying \gls{lmt} and \gls{mob} to the Automobile dataset, these methods cannot cope either with this larger dataset.

Consequently, \textit{glmtree}-SEM is only compared to the current performance. The combined scorecards have an overall performance of approximately 46 Gini points but they are not on the same ``scale'' since they were developed at different times. I rely on the Platt scaling method developed in~\cite{platt1999probabilistic} and~\cite{zadrozny2002transforming} and used in common \textit{machine learning} libraries such as Scikit-learn, to put all of them on the same scale by fitting a \gls{lr} between the observed labels $\gls{bby}$ and the scores outputted by each scorecard. After this procedure, overall performance jumps to approximately 55 Gini points which will be our baseline.

The \textit{glmtree}-SEM applied to this big dataset .

\bigskip

This chapter aimed at formalizing an old problem in \textit{Credit Scoring}, providing a literature review as well as a research idea for future work. As is often the case, practitioners have had good intuitions to deal with practical and theoretical requirements, such as performing clustering techniques, choosing segments empirically from the resulting visualization and fitting \gls{lr} on these.

However, situations can easily be imagined where such practices can fail, which is why other existing methods, that take into account the predictive task, were exemplified. Nevertheless, as in the best case scenario, practitioners would like to have an all-in-one tool that works with missing values and eventually performs quantization and interaction screening while guaranteeing the best predictive performance by embedding the learning of a segmentation in the predictive task of learning its \gls{lr}, a new method is proposed, based on an \gls{sem} algorithm, that was adapted to be usable with the \textit{glmdisc} method developed in the two preceding chapters.

On simulated data, it shows very promising results that aims at demonstrating the consistency of the approach. On real data from \gls{cacf}, other methods yielded disappointing results while \textit{glmtree}-SEM was able to compete with the current performance which required months of manual adjustments. By adding the quantization and interaction screening ability to this algorithm, as described in Section~\ref{sec:adding_quant}, we could easily imagine beating this \textit{ad hoc} segmentation by a significant margin.

\printbibliography[heading=subbibliography, title=References of Chapter 5]

