\chapter{Tree-structure segmentation for logistic regression} \label{chap6}

\epigraph{.}{.}

\minitoc


\textit{Nota Bene :} Ce chapitre s'inspire fortement ... \textcolor{red}{à adapter au moment de l'envoi du manuscrit}

\bigskip

\selectlanguage{english}

In Chapter~\ref{chap1}, and parallel to quantization, it was argued in Section~\ref{} tat, what is referred to as ``segmentation'' in the \textit{Credit Scoring} industry, could also be a straightforward solution to deal with missing values and outliers. The more theoretical justification of quantization, sketched in Section~\ref{}, was to achieve a good bias-variance tradeoff of the predictive task. This goal was embedded in the proposed quantization algorithm. Here again, the resulting segmentation and scorecards therein can be viewed as a single model for the whole population. In the next Section, we give some industrial context to the problem which is followed in Section~\ref{} by a literature review. Section~\ref{} reinterprets this problem, as promised, as a model selection problem for which a specific approach is design in subsequent Sections.


\section{Introduction}

\subsection{Context}

En matières de crédit à la consommation, les instituts financiers cherchent à automatiser la d´ecision de financement tout en ne sélectionnant que les clients susceptibles de rembourser ledit crédit. Depuis une quarantaine d’années, le Credit Scoring consiste à construire des modèles de classification supervisée $p_{\glssymbol{bth}}$ à partir des données demandées au clients $\glssymbol{bx} = (x_j)_1^d$ et de l’observation du remboursement des clients passés $y \in \{0, 1\}$. Historiquement, des scores différents sont développés sur des marchés (\textit{e.g.}\ grande distribution, électroménager, \dots) et/ou des produits (e.g. renouvelable, amortissable, \dots) et/ou des partenaires et/ou des profils clients différents dans l’esprit de la figure~\ref{fig:arbre}. Ce découpage est historique et relève d’un \textit{a priori}. On cherche ici à rationnaliser cette pratique en considérant le cluster d’appartenance du client comme un paramètre à optimiser. Si l’on note $K$ le nombre de scores à construire (inconnu) et $c = 1..K$ chaque score, correspondant à un cluster de clients, le mélange de régressions logistiques s’écrit : $p(y|x) = \sum_{c=1}^K p_{\glssymbol{bth}_c}(y|x, c)p(c|x)$, où l’on restreint $p(c|x)$ à prendre la forme de la figure~\ref{fig:arbre}, de telle sorte que le mélange n’est pas “flou” comme pour un modèle de mélange classique où la contribution de chaque classe est pondérée par sa probabilité. La
difficulté d’une approche directe réside dans cette contrainte discrète.


\tikzstyle{level 1}=[level distance=1.5cm, sibling distance=7cm]
\tikzstyle{level 2}=[level distance=1.5cm, sibling distance=4cm]
\tikzstyle{level 3}=[level distance=2cm, sibling distance=2cm]

\begin{figure}
\resizebox{\textwidth}{!}{
\centering
\begin{tikzpicture}
  [
    sibling distance        = 15em,
    level distance          = 5em,
    edge from parent/.style = {draw, -latex},
    every node/.style       = {font=\footnotesize},
    sloped
  ]
  \node [root] {\textcolor{black}{Clientèle}}
    child { node [dummy] {}
      child { node [dummy] {}
        child { node [env] {\textcolor{black}{$p_{\theta_1}(y|x)$}}
          edge from parent node [below] {Retraités} }
        child { node [env] {\textcolor{black}{$p_{\theta_2}(y|x)$}}
          edge from parent node [above] {Salariés} }
        child { node [env] {\textcolor{black}{$p_{\theta_3}(y|x)$}}
                edge from parent node [above] {Autres} }
        edge from parent node [above] {Crédit renouvelable} }
      child { node [env] {\textcolor{black}{$p_{\theta_4}(y|x)$}}
              edge from parent node [above, align=center]
                {Amortissable} }
              edge from parent node [above] {Electroménager} }
    child { node [dummy] {}
      child { node [dummy] {}
        child { node [env] {\textcolor{black}{$p_{\theta_5}(y|x)$}}
          edge from parent node [above] {Location} }
        child { node [env] {\textcolor{black}{$p_{\theta_6}(y|x)$}}
                edge from parent node [above] {Amortissable} }
        edge from parent node [above] {Fiat} }
      child { node [env] {\textcolor{black}{$p_{\theta_7}(y|x)$}}
              edge from parent node [above, align=center]
                {Kawasaki} }
              edge from parent node [above] {Automobile} };
\end{tikzpicture}
}
\caption{Cartographie simplifiée de la chaîne de construction des scores.} 
\label{fig:arbre}
\end{figure}




\subsection{In-house \textit{ad hoc} practice}





\section{Litterature review}


\subsection{Clustering methods}


\subsection{Direct approaches: logistic regression trees}



\section{Logistic regression trees as a combinatorial model selection problem}


\subsection{Cardinality example}


\subsection{Logistic regression tree selection}



\section{A mixture and latent feature-based relaxation}




\section{A stochastic estimation strategy}


\section{Numerical experiments}


\subsection{Empirical consistency on simulated data}

\subsection{Benchmark on \textit{Credit Scoring} data}




\bigskip

Ce chapitre.


\printbibliography[heading=subbibliography, title=References of Chapter 5]

