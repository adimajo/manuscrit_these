\chapter{Software} \label{app2}

\section{The R Statistical Software}

The vast majority of the code used throughout this manuscript has been done in \textsf{R}. Most experiments can be rerun from the Github repository of the manuscirpt at \url{https://www.github.com/adimajo/manuscrit_these}.

More information about the \textsf{R} Statistical Software, RStudio, and git, which I used extensively during the PhD, can be found respectively in~\cite{prog_r_avance,gillespie_lovelace_2016,git_course}.

%\subsection{The \rinline{glmdisc} package} \label{app2:glmdisc}
\subsection{The glmdisc package} \label{app2:glmdisc}

The \rinline{glmdisc} package can be found on Github at \url{https://www.github.com/adimajo/glmdisc}. It consists in the \textsf{R} implementation of the \textit{glmdisc} algorithm for discretizing continuous attributes, merging factor levels and introducing sparse pairwise interactions proposed in Chapters~\ref{chap5} and~\ref{chap6}.

\paragraph{Quick installation guide}

As the package is hosted on Github, a simple installation procedure is to get the \rinline{devtools} package and run:

\begin{rlisting}
devtools::install_github("adimajo/glmdisc", build_vignette = TRUE)
\end{rlisting}

The \rinline{build_vignette} argument ensures the package's vignette is installed as well.

Behind company proxies however, \rinline{devtools::install_github} might not work (contrary to \rinline{install.packages} if the proxy is well set up). A workaround is to get the \rinline{httr} package which allows to wrap the previous function call in \rinline{with_config(use_proxy(YOUR_PROXY_SETTINGS),...}).

\paragraph{Main functions}

Once installed, the \textsf{R} help and vignette detail the functioning of the package. Nevertheless, I should mention a few useful tips:

\begin{itemize}
\item The package's vignette can be obtained \textit{e.g.}\ by \rinline{vignette("glmdisc")}.
\item The main function is \rinline{glmdisc( )}, where (see \rinline{help(glmdisc)} for details).
\item Its main arguments are \rinline{predictors} and \rinline{labels} where \rinline{predictors} are assumed to be of type \rinline{num} or \rinline{factor}.
\item An option to seek for interactions (see Chapter~\ref{chap5}) is given as \rinline{interact = TRUE}.
\item For the moment, it is not possible to do Cross-Validation or to use a user-defined validation / test dataset(s).
\item However, arguments \rinline{validation}, \rinline{test} and \rinline{proportions} lets the user choose if the quantization / interactions are optimized on the validation set, if a test performance has to be reported, and what proportion(s) of the original data has to be used for each of these datasets, \textit{e.g.}\ \rinline{proportions = c(0.2,0.2)} splits \rinline{predictors} and \rinline{labels} into 60~\% training, 20~\% validation and 20~\% test.
\item To use a learnt quantization, and its associated \gls{lr} on a new set, the \rinline{predict} and \rinline{discretize} functions are provided.
\item If a presumable bug is encountered, an issue may be raised on the Github page of the package.
\end{itemize}

\subsection{Miscellaneous}

Apart from the \rinline{glmdisc} package, I produced a package named \rinline{scoring} for the purpose of \textit{Credit Scoring} practitioners, which contains the \rinline{glmdisc} package, the Reject Inference methods discussed thoroughly in Chapter~\ref{chap2} and detailed in Appendix~\ref{app1:reject}, enhances the discretization package containing, among others, the MDLP and $\chi^2$ discretization methods to which \textit{glmdisc} is compared in Chapter~\ref{chap4}, and the model to perform automatic segmentation discussed in Chapter~\ref{chap6}. The scoring package can be found at \url{https://www.github.com/adimajo/scoring}.

The figures that were generated by the combined used of \textsf{R} code and the \rinline{tikzDevice} package can be rerun and are located in the \verb|R_CODE_FIGURES| folder of the repository.

\section{The Python programming language}

Some experiments were performed in Python, both to benefit from implementations not available in \textsf{R} and to learn this rapidly-growing multi-purpose language which machine learning packages have ``catched up'' on the exhaustivity of the \textsf{R} framework.

%\subsection{The \pyinline{glmdisc} package}
\subsection{The glmdisc package}

The \textit{glmdisc}-SEM algorithm is available in Python, though in inferior state of development in comparison to the \textsf{R} implementation, at the following link: \url{https://www.github.com/adimajo/glmdisc_python}.

\paragraph{Quick installation guide}

As the package is hosted on Github, a simple installation procedure is to use pip.

\begin{bashlisting}
pip install --upgrade https://github.com/adimajo/glmdisc_python/archive/master.tar.gz
\end{bashlisting}

Again, behind company proxies, it might be useful to add the \bashinline{--proxy=http://username:password@server:port} option.

\paragraph{Main functions}

Once installed, the Python help can be browsed with the \pyinline{help} function. I tried to use the coding style of \pyinline{scikit-learn} as much as possible such that the quantization algorithm \textit{glmdisc} is here provided as a \pyinline{class}, which itself provides several methods among which \pyinline{fit} is appropriate to train the quantization.

\subsection{The glmdisc-NN notebooks} \label{app2:nn}

As mentioned in Chapter~\ref{chap4}, the implementation of \textit{glmdisc}-NN is straightforward in terms of neural network architecture. Therefore, all experiments involving \textit{glmdisc}-NN were performed in Jupyter Notebooks. The Notebooks for experiments on simulated data can be found in the \verb|PYTHON_NOTEBOOKS| folder of the repository.

The following snippet illustrates how straightforward the implementation of \textit{glmdisc}-NN is, as seen as a computational graph on Figure~\ref{fig:nn}.

\begin{pylisting}
# The data function which provides all inputs to create_model is not shown here for concision.

def create_model(x_quant,x_qual,x_qual_dummy,y,x_quant_test,x_qual_test,x_qual_dummy_test,y_test):
    """Creates and trains the proposed neural network architecture.
    Args:
       x_quant,x_qual,x_qual_dummy,y,x_quant_test,x_qual_test,x_qual_dummy_test,y_test - input data given by the data() function not shown here for concision.
    Returns:
       loss - the performance (here Gini on test sample) of the resulting best quantization
       model - the trained model
       predicted - the predicted probabilities on the test set using the best quantization (to compute confidence intervals)
    """
    
    def initialize_neural_net(m_quant,m_qual):
        """Initializes the neural network architecture for quantization.
	    Args:
	       m_quant - list of maximum number of categories per continuous feature
	       m_qual - list of maximum number of groups of levels per categorical feature
	    Returns:
            liste_inputs_quant, liste_layers_quant, liste_layers_quant_inputs - lists of inputs / layers for continuous features
            liste_inputs_qual, liste_layers_qual, liste_layers_qual_inputs - same for categorical features
	    """
	    
        liste_inputs_quant = [None] * d1
        liste_inputs_qual = [None] * d2

        liste_layers_quant = [None] * d1
        liste_layers_qual = [None] * d2

        liste_layers_quant_inputs = [None] * d1
        liste_layers_qual_inputs = [None] * d2

        for i in range(d1):
            liste_inputs_quant[i] = Input((1, ))
            liste_layers_quant[i] = Dense(m_quant[i], activation='softmax')
            liste_layers_quant_inputs[i] = liste_layers_quant[i](
                liste_inputs_quant[i])

        for i in range(d2):
            liste_inputs_qual[i] = Input((len(np.unique(x_qual[:, i])), ))
            if (len(np.unique(x_qual[:, i])) > m_qual[i]):
                liste_layers_qual[i] = Dense(
                m_qual[i], activation='softmax', use_bias=False)
            else:
                liste_layers_qual[i] = Dense(
                len(np.unique(x_qual[:, i])), activation='softmax', use_bias=False)

            liste_layers_qual_inputs[i] = liste_layers_qual[i](
                liste_inputs_qual[i])

        return ([
            liste_inputs_quant, liste_layers_quant, liste_layers_quant_inputs,
            liste_inputs_qual, liste_layers_qual, liste_layers_qual_inputs
        ])
    
    
    def from_layers_to_proba_training(d1,d2,liste_layers_quant,liste_layers_qual):
        """Computes q_(alpha) for training samples.
	    Args:
	       d1, d2 - number of continuous (resp. categorical) features
	       liste_layers_quant, liste_layers_qual - given by initialize_neural_net(...)
	    Returns:
			results - list of matrices of q_(alpha,i,j,h) on training samples
	    """

        results = [None] * (d1 + d2)

        for j in range(d1):
            results[j] = K.function([liste_layers_quant[j].input],
                                    [liste_layers_quant[j].output])(
                                        [x_quant[:, j, np.newaxis]])

        for j in range(d2):
            results[j + d1] = K.function([liste_layers_qual[j].input],
                                         [liste_layers_qual[j].output])(
                                             [liste_qual_arrays[j]])

        return (results)
    
    
    
    
    def from_weights_to_proba_test(d1,d2,m_quant,m_qual,history,x_quant_test,x_qual_test,n_test):
        """Computes q_(alpha) for test samples.
	    Args:
	       d1, d2 - number of continuous (resp. categorical) features
	       m_quant, m_qual - 
	       history - 
	       x_quant_test, x_qual_test, n_test - 
	    Returns:
			results - list of matrices of q_(alpha,i,j,h) on test samples
	    """
	    
        results = [None] * (d1 + d2)    

        for j in range(d1):
            results[j] = np.zeros((n_test,m_quant[j]))
            for i in range(m_quant[j]):
                results[j][:,i] = history.best_weights[j][1][i] + history.best_weights[j][0][0][i]*x_quant_test[:,j]


        for j in range(d2):
            results[j+d1] = np.zeros((n_test,history.best_weights[j+d1][0].shape[1]))
            for i in range(history.best_weights[j+d1][0].shape[1]):
                for k in range(n_test):
                    results[j+d1][k,i] = history.best_weights[j+d1][0][x_qual_test[k,j],i]

        return(results)
    
    
    def evaluate_disc(type,d1,d2,misc):
        """Evaluates the quality of a quantization.
	    Args:
	       type - train or test
	       d1, d2 - number of continuous (resp. categorical) features
	       misc - depends on type
	    Returns:
		  performance - for type="train" BIC; for type="test" Gini.
		  predicted - the resulting quantization of either train or test data depending on type.
	    """

        if type=="train":
            proba = from_layers_to_proba_training(d1,d2,misc[0],misc[1])
        else:
            proba = from_weights_to_proba_test(d1,d2,misc[0],misc[1],misc[2],misc[3],misc[4],misc[5])


        results = [None] * (d1 + d2)

        if type=="train":
            X_transformed = np.ones((n, 1))
        else:
            X_transformed = np.ones((n_test, 1))

        for j in range(d1 + d2):
            if type=="train":
                results[j] = np.argmax(proba[j][0], axis=1)
            else:
                results[j] = np.argmax(proba[j], axis=1)
            X_transformed = np.concatenate(
                (X_transformed, sk.preprocessing.OneHotEncoder(categories='auto',sparse=False,handle_unknown="ignore").fit_transform(
                    X=results[j].reshape(-1, 1))),
                axis=1)

        proposed_logistic_regression = sk.linear_model.LogisticRegression(
            fit_intercept=False, solver = "lbfgs", C=1e20, tol=1e-8, max_iter=50)


        if type=="train":
            proposed_logistic_regression.fit(X=X_transformed, y=y.reshape((n, )))
            performance = 2 * sk.metrics.log_loss(
              y,
              proposed_logistic_regression.predict_proba(X=X_transformed)[:, 1],
              normalize=False
          ) + proposed_logistic_regression.coef_.shape[1] * np.log(n)
            predicted = proposed_logistic_regression.predict_proba(X_transformed)[:,1]

        else:
            proposed_logistic_regression.fit(X=X_transformed, y=y_test.reshape((n_test, )))
            performance = 2*sk.metrics.roc_auc_score(y_test,proposed_logistic_regression.predict_proba(X_transformed)[:,1])-1
            predicted = proposed_logistic_regression.predict_proba(X_transformed)[:,1]

        return (performance, predicted)


    
    class LossHistory(Callback):
        """Callback for Keras. At each epoch, computes the performance of the proposed quantization."""

        def on_train_begin(self, logs={}):
            self.losses = []
            self.best_criterion = float("inf")
            self.best_outputs = []

        def on_epoch_end(self, batch, logs={}):
            self.losses.append(evaluate_disc("train",d1,d2,[liste_layers_quant,liste_layers_qual])[0])
            if self.losses[-1] < self.best_criterion:
                self.best_weights = []
                self.best_outputs = []
                self.best_criterion = self.losses[-1]
                for j in range(d1):
                    self.best_weights.append(liste_layers_quant[j].get_weights())
                    self.best_outputs.append(
                        K.function([liste_layers_quant[j].input],
                                   [liste_layers_quant[j].output])(
                                       [x_quant[:, j, np.newaxis]]))
                for j in range(d2):
                    self.best_weights.append(liste_layers_qual[j].get_weights())
                    self.best_outputs.append(
                        K.function([liste_layers_qual[j].input],
                                   [liste_layers_qual[j].output])(
                                       [liste_qual_arrays[j]]))
    
    
	# quant is the number of maximum intervals per continuous feature
	# it is the single user-defined parameter of our proposal    
    
    quant = 10
    qual = 5

    m_quant = [int(quant)] * d1
    m_qual = [int(qual)] * d2

	liste_inputs_quant, liste_layers_quant, liste_layers_quant_inputs, liste_inputs_qual, liste_layers_qual, liste_layers_qual_inputs = initialize_neural_net(m_quant,m_qual)

	# full_hidden is the concatenation of all component-wise layers
	
    full_hidden = concatenate(
        list(
          chain.from_iterable(
              [liste_layers_quant_inputs, liste_layers_qual_inputs])))
    output = Dense(1, activation='sigmoid')(full_hidden)
    model = Model(
      inputs=list(chain.from_iterable([liste_inputs_quant, liste_inputs_qual])),
      outputs=[output])  

    optim = optimizers.SGD(lr=10**-3)       
 
    model.compile(loss='binary_crossentropy', optimizer=optim, metrics=['accuracy'])

    history = LossHistory()
    
    model.fit(
      list(chain.from_iterable([list(x_quant.T), liste_qual_arrays])),
      y,
      epochs=600,
      batch_size=128,
      callbacks=history)

    n_test = x_quant_test.shape[0]
    performance, predicted = evaluate_disc("test",d1,d2,misc=[m_quant,m_qual,history,x_quant_test,x_qual_test,n_test])
  
    return {'loss': -performance, 'model': model, 'predicted': predicted}

\end{pylisting}
