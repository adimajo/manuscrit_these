\chapter{Software} \label{app2}

\section{The R Statistical Software}

The vast majority of the code used throughout this manuscript has been done in \textsf{R}. Most experiments can be rerun from the Github repository of the manuscirpt at \url{https://www.github.com/adimajo/manuscrit_these}.

More information about the \textsf{R} Statistical Software, RStudio, and git, which I used extensively during the PhD, can be found respectively in~\cite{}.

\subsection{The glmdisc package} \label{app2:glmdisc}

% Fonctions principales
The glmdisc package can be found on Github at \url{https://www.github.com/adimajo/glmdisc}. It consists in the \textsf{R} implementation of the \textit{glmdisc} algorithm for discretizing continuous attributes, merging factor levels and introducing sparse pairwise interactions proposed in Chapters~\ref{chap5} and~\ref{chap6}.

\paragraph{Quick installation guide}

As the package is hosted on Github, a simple installation procedure is to get the \rinline{devtools} package and run:

\begin{rlisting}
devtools::install_github("adimajo/glmdisc", build_vignette = TRUE)
\end{rlisting}

The \rinline{build_vignette} argument ensures the package's vignette is installed as well.

Behind company proxies however, \rinline{devtools::install_github} might not work (contrary to \rinline{install.packages} if the proxy is well set up). A workaround is to get the \rinline{httr} package which allows to wrap the previous function call in \rinline{with_config(use_proxy(YOUR_PROXY_SETTINGS),...}).

\paragraph{Main functions}

Once installed, the \textsf{R} help and vignette detail the functioning of the package. Nevertheless, I should mention a few 


\subsection{Miscellaneous}

Apart from the glmdisc package, I produced a package named scoring for the purpose of \textit{Credit Scoring} practitioners, which contains the glmdisc package, the Reject Inference methods discussed thoroughly in Chapter~\ref{chap2} and detailed in Appendix~\ref{app1:reject}, enhances the discretization package containing, among others, the MDLP and $\chi^2$ discretization methods to which \textit{glmdisc} is compared in Chapter~\ref{chap4}, and and the model to perform automatic segmentation discussed in Chapter~\ref{chap6}. The scoring package can be found at \url{https://www.github.com/adimajo/scoring}.

The figures that were generated by the combined used of \textsf{R} code and the \rinline{tikzDevice} package can be rerun and are located in the \verb|R_CODE_FIGURES| folder of the repository.

\section{The Python programming language}

Some experiments were performed in Python, both to benefit from implementations not available in \textsf{R} and to learn this rapidly-growing multi-purpose language which machine learning packages have ``catched up'' on the exhaustivity of the \textsf{R} framework.

\subsection{The glmdisc package}

The \textit{glmdisc}-SEM algorithm is available in Python, though in inferior state of development in comparison to the \textsf{R} implementation, at the following link: \url{https://www.github.com/adimajo/glmdisc_python}.

\paragraph{Quick installation guide}

As the package is hosted on Github, a simple installation procedure is to use pip.

\begin{bashlisting}
pip install --upgrade https://github.com/adimajo/glmdisc_python/archive/master.tar.gz
\end{bashlisting}

Again, behind company proxies, it might be useful to add the \bashinline{--proxy=http://username:password@server:port} option.

\paragraph{Main functions}

Once installed, the Python help.

\subsection{The glmdisc-NN notebooks} \label{app2:nn}

As mentioned in Chapter~\ref{chap4}, the implementation of \textit{glmdisc}-NN is straightforward in terms of neural network architecture. Therefore, all experiments involving \textit{glmdisc}-NN were performed in Jupyter Notebooks. The Notebooks for experiments on simulated data can be found in the \verb|PYTHON_NOTEBOOKS| folder of the repository.

The following snippet illustrates how straightforward the implementation of \textit{glmdisc}-NN is, as seen as a computational graph on Figure~\ref{fig:nn}.

\begin{pylisting}
# create_model is used to "feed" the hyperas package to optimize the hyperparameters of the resulting neural network.
# The data function which provides all inputs to create_model is not shown here for concision.

def create_model(x_quant,x_qual,x_qual_dummy,y,x_quant_test,x_qual_test,x_qual_dummy_test,y_test):
    """Creates and trains a neural network with hyperparameters in brackets {{}} to be used by hyperas.
    Args:
       x_quant,x_qual,x_qual_dummy,y,x_quant_test,x_qual_test,x_qual_dummy_test,y_test - input data given by the data() function not shown here for concision.
    Returns:
       loss - the performance (here Gini on test sample) of the resulting best quantization
       status - for hyperas
       model - the trained model
       predicted - the predicted probabilities on the test set using the best quantization (to compute confidence intervals)
    """
    
    def initialize_neural_net(m_quant,m_qual):
        """Initializes the neural network architecture for quantization.
	    Args:
	       m_quant - list of maximum number of categories per continuous feature
	       m_qual - list of maximum number of groups of levels per categorical feature
	    Returns:
            liste_inputs_quant, liste_layers_quant, liste_layers_quant_inputs - lists of inputs / layers for continuous features
            liste_inputs_qual, liste_layers_qual, liste_layers_qual_inputs - same for categorical features
	    """
	    
        liste_inputs_quant = [None] * d1
        liste_inputs_qual = [None] * d2

        liste_layers_quant = [None] * d1
        liste_layers_qual = [None] * d2

        liste_layers_quant_inputs = [None] * d1
        liste_layers_qual_inputs = [None] * d2

        for i in range(d1):
            liste_inputs_quant[i] = Input((1, ))
            liste_layers_quant[i] = Dense(m_quant[i], activation='softmax')
            liste_layers_quant_inputs[i] = liste_layers_quant[i](
                liste_inputs_quant[i])

        for i in range(d2):
            liste_inputs_qual[i] = Input((len(np.unique(x_qual[:, i])), ))
            if (len(np.unique(x_qual[:, i])) > m_qual[i]):
                liste_layers_qual[i] = Dense(
                m_qual[i], activation='softmax', use_bias=False)
            else:
                liste_layers_qual[i] = Dense(
                len(np.unique(x_qual[:, i])), activation='softmax', use_bias=False)

            liste_layers_qual_inputs[i] = liste_layers_qual[i](
                liste_inputs_qual[i])

        return ([
            liste_inputs_quant, liste_layers_quant, liste_layers_quant_inputs,
            liste_inputs_qual, liste_layers_qual, liste_layers_qual_inputs
        ])
    
    
    def from_layers_to_proba_training(d1,d2,liste_layers_quant,liste_layers_qual):
        """Computes q_(alpha) for training samples.
	    Args:
	       d1, d2 - number of continuous (resp. categorical) features
	       liste_layers_quant, liste_layers_qual - given by initialize_neural_net(...)
	    Returns:
			results - list of matrices of q_(alpha,i,j,h) on training samples
	    """

        results = [None] * (d1 + d2)

        for j in range(d1):
            results[j] = K.function([liste_layers_quant[j].input],
                                    [liste_layers_quant[j].output])(
                                        [x_quant[:, j, np.newaxis]])

        for j in range(d2):
            results[j + d1] = K.function([liste_layers_qual[j].input],
                                         [liste_layers_qual[j].output])(
                                             [liste_qual_arrays[j]])

        return (results)
    
    
    
    
    def from_weights_to_proba_test(d1,d2,m_quant,m_qual,history,x_quant_test,x_qual_test,n_test):
        """Computes q_(alpha) for test samples.
	    Args:
	       d1, d2 - number of continuous (resp. categorical) features
	       m_quant, m_qual - 
	       history - 
	       x_quant_test, x_qual_test, n_test - 
	    Returns:
			results - list of matrices of q_(alpha,i,j,h) on test samples
	    """
	    
        results = [None] * (d1 + d2)    

        for j in range(d1):
            results[j] = np.zeros((n_test,m_quant[j]))
            for i in range(m_quant[j]):
                results[j][:,i] = history.best_weights[j][1][i] + history.best_weights[j][0][0][i]*x_quant_test[:,j]


        for j in range(d2):
            results[j+d1] = np.zeros((n_test,history.best_weights[j+d1][0].shape[1]))
            for i in range(history.best_weights[j+d1][0].shape[1]):
                for k in range(n_test):
                    results[j+d1][k,i] = history.best_weights[j+d1][0][x_qual_test[k,j],i]

        return(results)
    
    
    def evaluate_disc(type,d1,d2,misc):
        """Evaluates the quality of a quantization.
	    Args:
	       type - train or test
	       d1, d2 - number of continuous (resp. categorical) features
	       misc - depends on type
	    Returns:
			performance - for type="train" BIC; for type="test" Gini.
			predicted - the resulting quantization of either train or test data depending on type.
	    """

        if type=="train":
            proba = from_layers_to_proba_training(d1,d2,misc[0],misc[1])
        else:
            proba = from_weights_to_proba_test(d1,d2,misc[0],misc[1],misc[2],misc[3],misc[4],misc[5])


        results = [None] * (d1 + d2)

        if type=="train":
            X_transformed = np.ones((n, 1))
        else:
            X_transformed = np.ones((n_test, 1))

        for j in range(d1 + d2):
            if type=="train":
                results[j] = np.argmax(proba[j][0], axis=1)
            else:
                results[j] = np.argmax(proba[j], axis=1)
            X_transformed = np.concatenate(
                (X_transformed, sk.preprocessing.OneHotEncoder(categories='auto',sparse=False,handle_unknown="ignore").fit_transform(
                    X=results[j].reshape(-1, 1))),
                axis=1)

        proposed_logistic_regression = sk.linear_model.LogisticRegression(
            fit_intercept=False, solver = "lbfgs", C=1e20, tol=1e-8, max_iter=50)


        if type=="train":
            proposed_logistic_regression.fit(X=X_transformed, y=y.reshape((n, )))
            performance = 2 * sk.metrics.log_loss(
              y,
              proposed_logistic_regression.predict_proba(X=X_transformed)[:, 1],
              normalize=False
          ) + proposed_logistic_regression.coef_.shape[1] * np.log(n)
            predicted = proposed_logistic_regression.predict_proba(X_transformed)[:,1]

        else:
            proposed_logistic_regression.fit(X=X_transformed, y=y_test.reshape((n_test, )))
            performance = 2*sk.metrics.roc_auc_score(y_test,proposed_logistic_regression.predict_proba(X_transformed)[:,1])-1
            predicted = proposed_logistic_regression.predict_proba(X_transformed)[:,1]

        return (performance, predicted)


    
    class LossHistory(Callback):
        """Callback for Keras. At each epoch, computes the performance of the proposed quantization.
	    """

        def on_train_begin(self, logs={}):
            self.losses = []
            self.best_criterion = float("inf")
            self.best_outputs = []

        def on_epoch_end(self, batch, logs={}):
            self.losses.append(evaluate_disc("train",d1,d2,[liste_layers_quant,liste_layers_qual])[0])
            if self.losses[-1] < self.best_criterion:
                self.best_weights = []
                self.best_outputs = []
                self.best_criterion = self.losses[-1]
                for j in range(d1):
                    self.best_weights.append(liste_layers_quant[j].get_weights())
                    self.best_outputs.append(
                        K.function([liste_layers_quant[j].input],
                                   [liste_layers_quant[j].output])(
                                       [x_quant[:, j, np.newaxis]]))
                for j in range(d2):
                    self.best_weights.append(liste_layers_qual[j].get_weights())
                    self.best_outputs.append(
                        K.function([liste_layers_qual[j].input],
                                   [liste_layers_qual[j].output])(
                                       [liste_qual_arrays[j]]))
    
    
	# quant is the number of maximum intervals per continuous feature
	# it is the single user-defined parameter of our proposal    
    
    quant = 10

    qual = 5

    m_quant = [int(quant)] * d1
    m_qual = [int(qual)] * d2

    global liste_qual_arrays
    liste_qual_arrays = [None] * d2
    cursor = 0
    for j in range(d2):
        liste_qual_arrays[j] = x_qual_dummy[:, cursor:(
            cursor + len(np.unique(x_qual[:, j])))]
        cursor += len(np.unique(x_qual[:, j]))

    global liste_qual_arrays_test
    liste_qual_arrays_test = [None] * d2
    cursor = 0
    for j in range(d2):
        liste_qual_arrays_test[j] = x_qual_dummy_test[:, cursor:(
            cursor + len(np.unique(x_qual_test[:, j])))]
        cursor += len(np.unique(x_qual_test[:, j]))
  
  
    liste_inputs_quant = [None] * d1
    liste_inputs_qual = [None] * d2

    global liste_layers_quant
    global liste_layers_qual

    liste_layers_quant = [None] * d1
    liste_layers_qual = [None] * d2

    liste_layers_quant_inputs = [None] * d1
    liste_layers_qual_inputs = [None] * d2

    for i in range(d1):
        liste_inputs_quant[i] = Input((1, ))
        liste_layers_quant[i] = Dense(m_quant[i], activation='softmax')
        liste_layers_quant_inputs[i] = liste_layers_quant[i](
          liste_inputs_quant[i])

    for i in range(d2):
        liste_inputs_qual[i] = Input((len(np.unique(x_qual[:, i])), ))
        if (len(np.unique(x_qual[:, i])) > m_qual[i]):
            liste_layers_qual[i] = Dense(
            m_qual[i], activation='softmax', use_bias=False)
        else:
            liste_layers_qual[i] = Dense(
            len(np.unique(x_qual[:, i])), activation='softmax', use_bias=False)

        liste_layers_qual_inputs[i] = liste_layers_qual[i](
        liste_inputs_qual[i])


    full_hidden = concatenate(
        list(
          chain.from_iterable(
              [liste_layers_quant_inputs, liste_layers_qual_inputs])))
    output = Dense(1, activation='sigmoid')(full_hidden)
    model = Model(
      inputs=list(chain.from_iterable([liste_inputs_quant, liste_inputs_qual])),
      outputs=[output])  

    
   
    choiceval = {{choice(['adam', 'sgd', 'rmsprop'])}}
    if choiceval == 'adam':
        optim = optimizers.Adam(lr={{choice([10**-3, 10**-2, 10**-1])}})
    elif choiceval == 'rmsprop':
        optim = optimizers.RMSprop(lr={{choice([10**-3, 10**-2, 10**-1])}})
    else:
        optim = optimizers.SGD(lr={{choice([10**-3, 10**-2, 10**-1])}})        
 
    model.compile(loss='binary_crossentropy', optimizer=optim, metrics=['accuracy'])

    history = LossHistory()

    callbacks = [
      ReduceLROnPlateau(
          monitor='loss',
          factor=0.5,
          patience=10,
          verbose=0,
          mode='auto',
          min_delta=0.0001,
          cooldown=0,
          min_lr=0), history
    ]
    model.fit(
      list(chain.from_iterable([list(x_quant.T), liste_qual_arrays])),
      y,
      epochs=600,
      batch_size={{choice([32,64,128])}},
      verbose=1,
      callbacks=callbacks)

    global n_test
    n_test = x_quant_test.shape[0]
    performance, predicted = evaluate_disc("test",d1,d2,misc=[m_quant,m_qual,history,x_quant_test,x_qual_test,n_test])
  

    K.clear_session()
    return {'loss': -performance, 'status': STATUS_OK, 'model': model, 'predicted': predicted}

\end{pylisting}
